{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Various helper functions.\n",
    "\n",
    "Note:\n",
    "    The functions are in alphabetical order.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def batch_kron(a: np.ndarray, b: np.ndarray):\n",
    "    \"\"\"\n",
    "    Batch Kronecker product: np.kron(a[i, :, :], b[i, :, :]) for all i.\n",
    "    \"\"\"\n",
    "    return np.einsum('bik,bjl->bijkl', a, b).reshape(a.shape[0], a.shape[1] * b.shape[1], a.shape[2] * b.shape[2])\n",
    "\n",
    "\n",
    "def cov2cov(matrices: np.ndarray):\n",
    "    \"\"\"\n",
    "    Convert the real representations of complex covariance matrices back to\n",
    "    complex representations.\n",
    "    \"\"\"\n",
    "    if matrices.ndim == 2:\n",
    "        # the case of diagonal matrices\n",
    "        n_mats, n_diag = matrices.shape\n",
    "        mats = np.zeros([n_mats, n_diag, n_diag])\n",
    "        for i in range(n_mats):\n",
    "            mats[i, :, :] = np.diag(matrices[i, :])\n",
    "    else:\n",
    "        mats = matrices\n",
    "\n",
    "    n_mats, rows, columns = mats.shape\n",
    "    row_half = rows // 2\n",
    "    column_half = columns // 2\n",
    "    covs = np.zeros((n_mats, row_half, column_half), dtype=complex)\n",
    "    for c in range(n_mats):\n",
    "        upper_left_block = mats[c, :row_half, :column_half]\n",
    "        upper_right_block = mats[c, :row_half, column_half:]\n",
    "        lower_left_block = mats[c, row_half:, :column_half]\n",
    "        lower_right_block = mats[c, row_half:, column_half:]\n",
    "        covs[c, :, :] = upper_left_block + lower_right_block + 1j * (lower_left_block - upper_right_block)\n",
    "    return covs\n",
    "\n",
    "\n",
    "def cplx2real(vec: np.ndarray, axis=0):\n",
    "    \"\"\"\n",
    "    Concatenate real and imaginary parts of vec along axis=axis.\n",
    "    \"\"\"\n",
    "    return np.concatenate([vec.real, vec.imag], axis=axis)\n",
    "\n",
    "\n",
    "def crandn(*arg, rng=np.random.default_rng()):\n",
    "    return np.sqrt(0.5) * (rng.standard_normal(arg) + 1j * rng.standard_normal(arg))\n",
    "\n",
    "\n",
    "# def crandn(*args, rng=None):\n",
    "#     # Create a TensorFlow random generator with the given seed\n",
    "#     if rng is None:\n",
    "#         tf.random.set_seed(1235428719812346)\n",
    "#         rng = tf.random.Generator.from_seed()\n",
    "    \n",
    "#     real_part = tf.sqrt(0.5) * rng.normal(shape=args)\n",
    "#     imag_part = tf.sqrt(0.5) * rng.normal(shape=args)\n",
    "\n",
    "#     print('dtype of tf.complex(real_part, imag_part): ', tf.complex(real_part, imag_part).dtype)\n",
    "    \n",
    "#     return tf.complex(real_part, imag_part)\n",
    "\n",
    "\n",
    "def kron_approx_sep_ls(\n",
    "    mats_A: np.ndarray,\n",
    "    init_C: np.ndarray,\n",
    "    rows_B: int,\n",
    "    cols_B: int,\n",
    "    iterations: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Approximate a matrix in terms of a Kronecker product of two matrices. The\n",
    "    array init_C is an initialization for the matrix C and will be overwritten.\n",
    "    If it is structured, for example, if it is positive definite, then the\n",
    "    returned matrices B and C will have the same structure. Section 5 of the\n",
    "    source explains what kind of structure can be used.\n",
    "\n",
    "    Note:\n",
    "        This corresponds to Framework 2 in Section 4 in the source.\n",
    "\n",
    "    Source:\n",
    "        \"Approximation with Kronecker Products\"\n",
    "        by Van Loan, Pitsianis\n",
    "        https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.42.1924&rep=rep1&type=pdf\n",
    "    \"\"\"\n",
    "    if mats_A.ndim == 2:\n",
    "        mats_A = np.expand_dims(mats_A, 0)\n",
    "        init_C = np.expand_dims(init_C, 0)\n",
    "\n",
    "    mats_B = np.zeros((mats_A.shape[0], rows_B, cols_B), dtype=mats_A.dtype)\n",
    "    mats_C = init_C\n",
    "    rows_C, cols_C = mats_C.shape[-2:]\n",
    "\n",
    "    # Extract the blocks A_ij: Split the 3d array of shape (n, rows_B * rows_C, cols_B * cols_C) into a 5d array of\n",
    "    # shape (n, rows_B, cols_B, rows_C, cols_C) where [:, i, j, :, :] corresponds to the blocks A_ij in equation (2).\n",
    "    blocks_A = np.zeros((mats_A.shape[0], rows_B, cols_B, rows_C, cols_C), dtype=mats_A.dtype)\n",
    "    for i, block_row in enumerate(np.split(mats_A, rows_B, axis=-2)):\n",
    "        for j, block in enumerate(np.split(block_row, cols_B, axis=-1)):\n",
    "            blocks_A[:, i, j, :, :] = block\n",
    "\n",
    "    # Extract the blocks Ahat_ij: Split the 3d array of shape (n, rows_B * rows_C, cols_B * cols_C) into a 5d array of\n",
    "    # shape (n, rows_C, cols_C, rows_B, cols_B) where [:, i, j, :, :] corresponds to the blocks Ahat_ij in equation (4).\n",
    "    blocks_Ahat = np.zeros((mats_A.shape[0], rows_C, cols_C, rows_B, cols_B), dtype=mats_A.dtype)\n",
    "    for i in range(rows_C):\n",
    "        for j in range(cols_C):\n",
    "            blocks_Ahat[:, i, j, :, :] = \\\n",
    "                mats_A[:, i: i + (rows_B - 1) * rows_C + 1: rows_C, j: j + (cols_B - 1) * cols_C + 1: cols_C]\n",
    "\n",
    "    beta_or_gamma = np.zeros((mats_A.shape[0], 1, 1), dtype=mats_A.dtype)\n",
    "\n",
    "    def project(blocks_ij, b_or_c, c_or_b_out):\n",
    "        \"\"\"\n",
    "        For every block A_ij (or Ahat_ij), compute equation (8) (or (9)) in Theorem 4.1.\n",
    "        \"\"\"\n",
    "        np.einsum('ijklm,ilm->ijk', blocks_ij, b_or_c, out=c_or_b_out)\n",
    "        np.einsum('ijk,ijk->i', b_or_c, b_or_c, out=beta_or_gamma[:, 0, 0])\n",
    "        c_or_b_out /= beta_or_gamma\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        project(blocks_A, mats_C, mats_B)\n",
    "        project(blocks_Ahat, mats_B, mats_C)\n",
    "\n",
    "    return mats_B, mats_C\n",
    "\n",
    "\n",
    "def kron_approx_svd(mats_A: np.ndarray, rows_B: int, cols_B: int, rows_C: int, cols_C: int):\n",
    "    r\"\"\"\n",
    "    Approximate a matrix in terms of a Kronecker product of two matrices.\n",
    "\n",
    "    Note:\n",
    "        Given a matrix A, find matrices B and C of shapes (rows_B, cols_B) and\n",
    "        (rows_C, cols_C) such that \\| A - B \\otimes C \\|_F is minimized.\n",
    "        If A is structured, e.g., symmetric or positive definite, the function\n",
    "        kron_approx_sep_ls can be used.\n",
    "\n",
    "    Source:\n",
    "        \"Approximation with Kronecker Products\"\n",
    "        by Van Loan, Pitsianis\n",
    "        https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.42.1924&rep=rep1&type=pdf\n",
    "    \"\"\"\n",
    "    if mats_A.ndim == 2:\n",
    "        mats_A = np.expand_dims(mats_A, 0)\n",
    "    n, rows_A, cols_A = mats_A.shape\n",
    "    if rows_B * rows_C != rows_A:\n",
    "        raise ValueError(f'rows_B*rows_C = {rows_A} is required, but rows_B*rows_C = {rows_B*rows_C}')\n",
    "    if cols_B * cols_C != cols_A:\n",
    "        raise ValueError(f'cols_B*cols_C = {cols_A} is required, but cols_B*cols_C = {cols_B*cols_C}')\n",
    "\n",
    "    def block(A, i, j):\n",
    "        \"\"\"\n",
    "        Extract block A_ij from the matrix A. Every block A_ij has shape\n",
    "        (rows_C, cols_C) and there are rows_B * cols_B such blocks.\n",
    "        \"\"\"\n",
    "        return A[i * rows_C: (i + 1) * rows_C, j * cols_C: (j + 1) * cols_C]\n",
    "\n",
    "    out_B = np.zeros((n, rows_B, cols_B), dtype=mats_A.dtype)\n",
    "    out_C = np.zeros((n, rows_C, cols_C), dtype=mats_A.dtype)\n",
    "    for ni in range(n):\n",
    "        # the following implements equation (5)\n",
    "        rearranged_A = np.zeros((rows_B * cols_B, rows_C * cols_C), dtype=mats_A.dtype)\n",
    "        for j in range(cols_B):\n",
    "            # extract the matrix A_j\n",
    "            rearranged_A[j * rows_B: (j + 1) * rows_B, :] = np.concatenate(\n",
    "                [block(mats_A[ni, :, :], i, j).flatten('F')[np.newaxis, :] for i in range(rows_B)],\n",
    "                axis=0\n",
    "            )\n",
    "        u, s, vh = np.linalg.svd(rearranged_A)\n",
    "        out_B[ni, :, :] = u[:, 0].reshape(rows_B, cols_B, order='F') * s[0]\n",
    "        out_C[ni, :, :] = vh[0, :].reshape(rows_C, cols_C, order='F')\n",
    "    return out_B, out_C\n",
    "\n",
    "\n",
    "def kron_real(mats1: np.ndarray, mats2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Assuming mats1 and mats2 are real representations of complex covariance\n",
    "    matrices, compute the real representation of the Kronecker product of the\n",
    "    complex covariance matrices.\n",
    "    \"\"\"\n",
    "    if mats1.ndim != mats2.ndim:\n",
    "        raise ValueError(\n",
    "            'The two arrays need to have the same number of dimensions, '\n",
    "            f'but we have mats1.ndim = {mats1.ndim} and mats2.ndim = {mats2.ndim}.'\n",
    "        )\n",
    "    if mats1.ndim == 2:\n",
    "        mats1 = np.expand_dims(mats1, 0)\n",
    "        mats2 = np.expand_dims(mats2, 0)\n",
    "\n",
    "    n = mats1.shape[0]\n",
    "    rows1, cols1 = mats1.shape[-2:]\n",
    "    rows2, cols2 = mats2.shape[-2:]\n",
    "    row_half1 = rows1 // 2\n",
    "    column_half1 = cols1 // 2\n",
    "    row_half2 = rows2 // 2\n",
    "    column_half2 = cols2 // 2\n",
    "    rows3 = 2 * row_half1 * row_half2\n",
    "    cols3 = 2 * column_half1 * column_half2\n",
    "\n",
    "    out_kron_prod = np.zeros((n, rows3, cols3))\n",
    "    for i in range(n):\n",
    "        A1 = mats1[i, :row_half1, :column_half1]\n",
    "        B1 = mats1[i, :row_half1, column_half1:]\n",
    "        C1 = mats1[i, row_half1:, :column_half1]\n",
    "        D1 = mats1[i, row_half1:, column_half1:]\n",
    "\n",
    "        A2 = mats2[i, :row_half2, :column_half2]\n",
    "        B2 = mats2[i, :row_half2, column_half2:]\n",
    "        C2 = mats2[i, row_half2:, :column_half2]\n",
    "        D2 = mats2[i, row_half2:, column_half2:]\n",
    "\n",
    "        A = np.kron(A1 + D1, A2 + D2)\n",
    "        D = -np.kron(C1 - B1, C2 - B2)\n",
    "        B = -np.kron(A1 + D1, C2 - B2)\n",
    "        C = np.kron(C1 - B1, A2 + D2)\n",
    "\n",
    "        A = 0.5 * (A + D)\n",
    "        D = A\n",
    "        B = 0.5 * (B - C)\n",
    "        C = -B\n",
    "\n",
    "        out_kron_prod[i, :, :] = np.concatenate(\n",
    "            (np.concatenate((A, B), axis=1), np.concatenate((C, D), axis=1)),\n",
    "            axis=0\n",
    "        )\n",
    "    return np.squeeze(out_kron_prod)\n",
    "\n",
    "\n",
    "def mat2bsc(mat: np.ndarray):\n",
    "    \"\"\"\n",
    "    Arrange the real and imaginary parts of a complex matrix mat in block-\n",
    "    skew-circulant form.\n",
    "\n",
    "    Source:\n",
    "        See https://ieeexplore.ieee.org/document/7018089.\n",
    "    \"\"\"\n",
    "    upper_half = np.concatenate((mat.real, -mat.imag), axis=-1)\n",
    "    lower_half = np.concatenate((mat.imag, mat.real), axis=-1)\n",
    "    return np.concatenate((upper_half, lower_half), axis=-2)\n",
    "\n",
    "\n",
    "def real2real(mats):\n",
    "    re = np.real(mats)\n",
    "    im = np.imag(mats)\n",
    "    rows = mats.shape[1]\n",
    "    cols = mats.shape[2]\n",
    "    out = np.zeros([mats.shape[0], 2*rows, 2*cols])\n",
    "    out[:, :rows, :cols] = 0.5*re\n",
    "    out[:, rows:, cols:] = 0.5*re\n",
    "    return out\n",
    "\n",
    "\n",
    "def imag2imag(mats):\n",
    "    im = np.real(mats)\n",
    "    rows = mats.shape[1]\n",
    "    cols = mats.shape[2]\n",
    "    out = np.zeros([mats.shape[0], 2*rows, 2*cols])\n",
    "    #out[:, :rows, :cols] = 0.5*re\n",
    "    for i in range(mats.shape[0]):\n",
    "        out[i, :rows, cols:] = 0.5*im[i,:,:].T\n",
    "        out[i, rows:, :cols] = 0.5*im[i,:,:]\n",
    "    #out[:, rows:, cols:] = 0.5*re\n",
    "    return out\n",
    "\n",
    "\n",
    "def nmse(actual: np.ndarray, desired: np.ndarray):\n",
    "    \"\"\"\n",
    "    Mean squared error between actual and desired divided by the total number\n",
    "    of elements.\n",
    "    \"\"\"\n",
    "    mse = 0\n",
    "    for i in range(actual.shape[0]):\n",
    "        mse += np.linalg.norm(actual - desired) ** 2 / np.linalg.norm(desired) ** 2\n",
    "    return mse / actual.shape[0]\n",
    "    #return np.sum(np.abs(actual - desired) ** 2) / desired.size\n",
    "\n",
    "\n",
    "def real2cplx(vec: np.ndarray, axis=0):\n",
    "    \"\"\"\n",
    "    Assume vec consists of concatenated real and imaginary parts. Return the\n",
    "    corresponding complex vector. Split along axis=axis.\n",
    "    \"\"\"\n",
    "    re, im = np.split(vec, 2, axis=axis)\n",
    "    return re + 1j * im\n",
    "\n",
    "\n",
    "def sec2hours(seconds: float):\n",
    "    \"\"\"\"\n",
    "    Convert a number of seconds to a string h:mm:ss.\n",
    "    \"\"\"\n",
    "    # hours\n",
    "    h = seconds // 3600\n",
    "    # remaining seconds\n",
    "    r = seconds % 3600\n",
    "    return '{:.0f}:{:02.0f}:{:02.0f}'.format(h, r // 60, r % 60)\n",
    "\n",
    "def check_random_state(seed):\n",
    "    import numbers\n",
    "    \"\"\"Turn seed into a np.random.RandomState instance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : None, int or instance of RandomState\n",
    "        If seed is None, return the RandomState singleton used by np.random.\n",
    "        If seed is an int, return a new RandomState instance seeded with seed.\n",
    "        If seed is already a RandomState instance, return it.\n",
    "        Otherwise raise ValueError.\n",
    "    \"\"\"\n",
    "    if seed is None or seed is np.random:\n",
    "        return np.random.mtrand._rand\n",
    "    if isinstance(seed, numbers.Integral):\n",
    "        return np.random.RandomState(seed)\n",
    "    if isinstance(seed, np.random.RandomState):\n",
    "        return seed\n",
    "    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'\n",
    "                     ' instance' % seed)\n",
    "\n",
    "def print_dict(dict: dict, entries_per_row: int=1):\n",
    "    \"\"\"Print the keys and values of dictionary dict.\"\"\"\n",
    "    if entries_per_row < 1:\n",
    "        raise ValueError(f'The number of entries per row needs to be >= 1 but is {entries_per_row}')\n",
    "    for c, (key, value) in enumerate(dict.items()):\n",
    "        if c % entries_per_row == 0 and c > 0:\n",
    "            print()\n",
    "        else:\n",
    "            c > 0 and print(' | ', end='')\n",
    "        print('{}: {}'.format(key, value), end='')\n",
    "    print()\n",
    "\n",
    "\n",
    "def dft_matrix(n_antennas, n_grid):\n",
    "    grid = np.linspace(-1, 1, n_grid + 1)[:n_grid]\n",
    "\n",
    "    d = 1 / np.sqrt(n_antennas) * np.exp(1j * np.pi * np.outer(np.arange(n_antennas), grid.conj().T))\n",
    "    return d\n",
    "\n",
    "#def dist_fac(n_bits: int):\n",
    "#    \"\"\"\"Compute distortion factor for an arbitrary number of bits.\"\"\"\n",
    "#    if n_bits == 1:\n",
    "#        raise ValueError('The distortion factor for 1-bit is known in closed form.')\n",
    "#    else:\n",
    "#        return n_bits * 2**(-2*n_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scm_channel(\n",
    "    angles,\n",
    "    weights,\n",
    "    n_coherence,\n",
    "    n_antennas,\n",
    "    sigma=2.0,\n",
    "    rng=np.random.default_rng()\n",
    "):\n",
    "    (h, t) = chan_from_spectrum(n_coherence, n_antennas, angles, weights, sigma, rng=rng)\n",
    "    return h, t\n",
    "\n",
    "\n",
    "def spectrum(u, angles, weights, sigma=2.0):\n",
    "    u = (u + np.pi) % (2 * np.pi) - np.pi\n",
    "    theta = np.degrees(np.arcsin(u / np.pi))\n",
    "    v = _laplace(theta, angles, weights, sigma) \\\n",
    "        + _laplace(180 - theta, angles, weights, sigma)\n",
    "\n",
    "    return np.degrees(2 * np.pi * v / np.sqrt(np.pi ** 2 - u ** 2))\n",
    "\n",
    "\n",
    "def _laplace(theta, angles, weights, sigma=2.0):\n",
    "    # The variance \\sigma^2 of a Laplace density is \\sigma^2 = 2 * scale_parameter^2.\n",
    "    # Hence, the standard deviation \\sigma is \\sigma = sqrt(2) * scale_parameter.\n",
    "    # The scale_parameter determines the Laplace density.\n",
    "    # For an angular spread (AS) given in terms of a standard deviation \\sigma\n",
    "    # the scale parameter thus needs to be computed as scale_parameter = \\sigma / sqrt(2)\n",
    "    scale_parameter = sigma / np.sqrt(2)\n",
    "    x_shifted = np.outer(theta, np.ones(angles.size)) - angles\n",
    "    x_shifted = (x_shifted + 180) % 360 - 180\n",
    "    v = weights / (2 * scale_parameter) * np.exp(-np.absolute(x_shifted) / scale_parameter)\n",
    "    return v.sum(axis=1)\n",
    "\n",
    "# def spectrum(u, angles, weights, sigma=2.0):\n",
    "#     pi = tf.constant(np.pi, dtype=tf.float32)\n",
    "#     u = (u + pi) % (2 * pi) - pi\n",
    "#     theta = (180.0 / pi) * tf.math.asin(u / pi)\n",
    "#     v = _laplace(theta, angles, weights, sigma) + _laplace(180 - theta, angles, weights, sigma)\n",
    "\n",
    "#     return (360.0 / pi) * (2 * pi * v / tf.sqrt(pi**2 - u**2))\n",
    "\n",
    "\n",
    "# def _laplace(theta, angles, weights, sigma=2.0):\n",
    "#     # The variance \\sigma^2 of a Laplace density is \\sigma^2 = 2 * scale_parameter^2.\n",
    "#     # Hence, the standard deviation \\sigma is \\sigma = sqrt(2) * scale_parameter.\n",
    "#     # The scale_parameter determines the Laplace density.\n",
    "#     # For an angular spread (AS) given in terms of a standard deviation \\sigma\n",
    "#     # the scale parameter thus needs to be computed as scale_parameter = \\sigma / sqrt(2)\n",
    "#     scale_parameter = sigma / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "    \n",
    "#     theta = tf.matmul(tf.reshape(theta, (theta.shape[0], 1)), tf.reshape(tf.ones_like(angles), (1, angles.shape[0])))\n",
    "\n",
    " \n",
    "#     # Calculate x_shifted using tf.matmul\n",
    "#     x_shifted = theta - tf.reshape(angles, (1, angles.shape[0]))\n",
    "\n",
    "#     x_shifted = theta - angles\n",
    "#     x_shifted = (x_shifted + 180) % 360 - 180\n",
    "#     print('x_shifted: ', x_shifted.shape)\n",
    "#     print('scale_parameter: ', scale_parameter.shape)\n",
    "#     print('weights: ', weights.shape)\n",
    "#     print('x_shifted: ', tf.exp(-tf.abs(x_shifted) / scale_parameter).shape)\n",
    "#     v = (weights / (2 * scale_parameter)) * tf.exp(-tf.abs(x_shifted) / scale_parameter)\n",
    "#     return tf.reduce_sum(v, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chan_from_spectrum(\n",
    "    n_coherence,\n",
    "    n_antennas,\n",
    "    angles,\n",
    "    weights,\n",
    "    sigma=2.0,\n",
    "    rng=np.random.default_rng()\n",
    "):\n",
    "\n",
    "    o_f = 100  # oversampling factor (ideally, would use continuous freq. spectrum...)\n",
    "    n_freq_samples = o_f * n_antennas\n",
    "\n",
    "    # Sample the spectrum which is defined in equation (78) with epsilon, try\n",
    "    # to avoid sampling at -pi and pi, thus avoiding dividing by zero.\n",
    "    epsilon = 1 / 3\n",
    "    lattice = np.arange(epsilon, n_freq_samples+epsilon) / n_freq_samples * 2 * np.pi - np.pi #sampled between -pi,+pi\n",
    "    fs = spectrum(lattice, angles, weights, sigma)\n",
    "    fs = np.reshape(fs, [len(fs), 1])\n",
    "\n",
    "    # Avoid instabilities due to almost infinite energy at some frequencies\n",
    "    # (this should only happen at \"endfire\" of a uniform linear array where --\n",
    "    # because of the arcsin-transform -- the angular psd grows to infinity).\n",
    "    almost_inf_threshold = np.max([1, n_freq_samples])  # use n_freq_samples as threshold value...\n",
    "    almost_inf_freqs = np.absolute(fs) > almost_inf_threshold\n",
    "    # this should not/only rarely be entered due to the epsilon above; one might even want to increase the threshold\n",
    "    # to, e.g., 30 * almost_inf_threshold\n",
    "\n",
    "    # if any(np.absolute(fs) > 20 * almost_inf_threshold):\n",
    "    #     print(\"almost inf: \", fs[almost_inf_freqs])\n",
    "\n",
    "    fs[almost_inf_freqs] = almost_inf_threshold  # * np.exp(1j * np.angle(fs[almost_inf_freqs])) # only real values\n",
    "\n",
    "    if np.sum(fs) > 0:\n",
    "        fs = fs / np.sum(fs) * n_freq_samples  # normalize energy\n",
    "\n",
    "    x = crandn(n_freq_samples, n_coherence, rng=rng)\n",
    "\n",
    "    h = np.fft.ifft(np.sqrt(fs)*x, axis=0) * np.sqrt(n_freq_samples)\n",
    "    h = h[0:n_antennas, :]\n",
    "\n",
    "    # t is the first row of the covariance matrix of h (which is Toeplitz and Hermitian)\n",
    "    t = np.fft.fft(fs, axis=0) / n_freq_samples\n",
    "    t = t[0:n_antennas]\n",
    "    t = np.reshape(t, n_antennas)\n",
    "\n",
    "    return h.T, t\n",
    "\n",
    "\n",
    "# def chan_from_spectrum(\n",
    "#     n_coherence,\n",
    "#     n_antennas,\n",
    "#     angles,\n",
    "#     weights,\n",
    "#     sigma=2.0,\n",
    "#     rng=None\n",
    "# ):\n",
    "#     # Set random seed for TensorFlow\n",
    "\n",
    "#     if rng is None:\n",
    "#         tf.random.set_seed(1235428719812346)\n",
    "#         rng = tf.random.Generator.from_seed()\n",
    "        \n",
    "\n",
    "#     o_f = 100  # oversampling factor (ideally, would use continuous freq. spectrum...)\n",
    "#     n_freq_samples = o_f * n_antennas\n",
    "\n",
    "#     # Sample the spectrum using TensorFlow\n",
    "#     epsilon = 1 / 3\n",
    "#     lattice = tf.range(epsilon, n_freq_samples + epsilon, dtype=tf.float32) / n_freq_samples * 2 * np.pi - np.pi\n",
    "#     fs = spectrum(lattice, angles, weights, sigma)  # Assuming you have a spectrum function\n",
    "    \n",
    "#     # Reshape fs\n",
    "#     fs = tf.reshape(fs, [len(fs), 1])\n",
    "\n",
    "#     # Avoid instabilities due to almost infinite energy at some frequencies\n",
    "#     almost_inf_threshold = tf.maximum(1.0, tf.cast(n_freq_samples, dtype=tf.float32))\n",
    "#     almost_inf_freqs = tf.abs(fs) > almost_inf_threshold\n",
    "\n",
    "#     # Set values for almost infinite frequencies\n",
    "#     fs = tf.where(almost_inf_freqs, almost_inf_threshold, fs)\n",
    "\n",
    "#     # Normalize energy\n",
    "#     fs = fs / tf.reduce_sum(fs) * n_freq_samples\n",
    "\n",
    "#     # Generate complex random numbers in TensorFlow\n",
    "#     x = crandn(n_freq_samples, n_coherence, rng=rng)\n",
    "\n",
    "#     print('dtype of tf.cast(tf.math.sqrt(n_freq_sampöles), dtype=tf.complex64): ', tf.math.sqrt(tf.cast(n_freq_samples, dtype=tf.complex64)).dtype)\n",
    "#     print('dtype of x: ', x.dtype)\n",
    "#     print('dtype of multiplication: ', (tf.cast(tf.math.sqrt(fs), dtype=tf.complex64) * x).dtype)\n",
    "\n",
    "#     # Compute channel response in TensorFlow\n",
    "#     h = tf.signal.ifft(tf.cast(tf.math.sqrt(fs), dtype=tf.complex64) * x) * tf.math.sqrt(tf.cast(n_freq_samples, dtype=tf.complex64))\n",
    "#     h = h[0:n_antennas, :]\n",
    "\n",
    "#     # Compute t as the first row of the covariance matrix of h\n",
    "#     t = tf.signal.fft(tf.cast(fs, dtype=tf.complex64)) / n_freq_samples\n",
    "#     t = t[0:n_antennas]\n",
    "#     t = tf.reshape(t, [n_antennas])\n",
    "\n",
    "#     return tf.transpose(h), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCMMulti:\n",
    "    \"\"\"Class to build a multi path channel model.\n",
    "\n",
    "    This class defines a multi path channel model.\n",
    "\n",
    "    Public Methods:\n",
    "\n",
    "    Instance Variables:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path_sigma=2.0, n_path=3):\n",
    "        \"\"\"Initialize multi path channel model.\n",
    "\n",
    "        First, initialise all variables belonging to the multi path channel model.\n",
    "        \"\"\"\n",
    "        self.path_sigma = path_sigma\n",
    "        self.n_path = n_path\n",
    "\n",
    "    def generate_channel(\n",
    "        self,\n",
    "        n_batches,\n",
    "        n_coherence,\n",
    "        n_antennas,\n",
    "        rng=np.random.default_rng()\n",
    "    ):\n",
    "        \"\"\"Generate multi path model parameters.\n",
    "\n",
    "        Returns:\n",
    "            A tuple (h, t) consisting of channels h with\n",
    "                h.shape = (n_batches, n_coherence, n_antennas)\n",
    "            and the first rows t of the covariance matrices with\n",
    "                t.shape = (n_batches, n_antennas)\n",
    "        \"\"\"\n",
    "        h = np.zeros([n_batches, n_coherence, n_antennas], dtype=np.complex64)\n",
    "        t = np.zeros([n_batches, n_antennas], dtype=np.complex64)\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            gains = rng.random(self.n_path)\n",
    "            gains = gains / np.sum(gains, axis=0)\n",
    "            angles = (rng.random(self.n_path) - 0.5) * 180\n",
    "\n",
    "            h[i, :, :], t[i, :] = scm_channel(angles, gains, n_coherence, n_antennas, self.path_sigma, rng=rng)\n",
    "\n",
    "        return h, t\n",
    "\n",
    "    # def generate_channel(\n",
    "    #     self,\n",
    "    #     n_batches,\n",
    "    #     n_coherence,\n",
    "    #     n_antennas,\n",
    "    #     rng=None\n",
    "    # ):\n",
    "\n",
    "    #     if rng is None:\n",
    "    #         tf.random.set_seed(1235428719812346)\n",
    "    #         rng = tf.random.Generator.from_seed()\n",
    "        \n",
    "    #     # n_batches is a symbolic tensor and i want to convert it to a tf.Tensor with the value of batch_size that i define in the main function\n",
    "    #     n_batches = tf.convert_to_tensor(n_batches, dtype=tf.int32)\n",
    "\n",
    "    #     print('n_batches: ', n_batches)\n",
    "\n",
    "    #     h = tf.zeros([n_batches, n_coherence, n_antennas], dtype=tf.complex64)\n",
    "    #     t = tf.zeros([n_batches, n_antennas], dtype=tf.complex64)\n",
    "\n",
    "    #     for i in range(n_batches):\n",
    "    #         gains = rng.uniform(shape=(self.n_path,))\n",
    "    #         gains = gains / tf.reduce_sum(gains)\n",
    "    #         angles = (rng.uniform(shape=(self.n_path,)) - 0.5) * 180.0\n",
    "\n",
    "    #         # Assuming you have a scm_channel function, replace with your actual implementation\n",
    "    #         h_i, t_i = scm_channel(angles, gains, n_coherence, n_antennas, self.path_sigma, rng=rng)\n",
    "\n",
    "    #         print('h_i: ', h_i.shape)\n",
    "    #         print('h: ', h.shape)\n",
    "    #         print('t_i: ', t_i.shape)\n",
    "\n",
    "    #         h = tf.tensor_scatter_nd_add(h, [[i]], h_i)\n",
    "    #         t = tf.tensor_scatter_nd_add(t, [[i]], t_i)\n",
    "\n",
    "    #     return h, t\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'path_sigma': self.path_sigma,\n",
    "            'n_path': self.n_path\n",
    "        }\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import toeplitz\n",
    "def channel_generation(batch_size, n_coherences, n_antennas):\n",
    "    \"\"\"\n",
    "    SIMO version.\n",
    "    \"\"\"\n",
    "    path_sigma = 2.0\n",
    "    n_path = 1\n",
    "    channel = SCMMulti(path_sigma=path_sigma, n_path=n_path)\n",
    "\n",
    "    # generate channel samples with a certain batch size\n",
    "    rng = np.random.default_rng(1235428719812346)\n",
    "\n",
    "    h, t = channel.generate_channel(batch_size, n_coherences, n_antennas, rng)\n",
    "        \n",
    "    # Initialize an empty array to store the covariance matrices\n",
    "    C = np.empty((batch_size, n_antennas, n_antennas), dtype=np.complex64)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Calculate the covariance matrix for each sample\n",
    "        C[i, :, :] = np.transpose(toeplitz(t[i, :]))\n",
    "    \n",
    "    # print('Generated ' + str(batch_size) + ' SIMO channel samples of size ' + str(n_antennas) + 'x1.')\n",
    "    \n",
    "    return h, C\n",
    "\n",
    "\n",
    "# def channel_generation(batch_size, n_coherences, n_antennas, seed=1235428719812346):\n",
    "#     path_sigma = 2.0\n",
    "#     n_path = 256\n",
    "#     # Set random seed for TensorFlow\n",
    "#     tf.random.set_seed(seed)\n",
    "\n",
    "#     # Create SCMMulti instance\n",
    "#     channel = SCMMulti(path_sigma=path_sigma, n_path=n_path)\n",
    "\n",
    "#     # Generate channel samples with a certain batch size\n",
    "#     rng = tf.random.Generator.from_seed(seed)\n",
    "#     h, t = channel.generate_channel(batch_size, n_coherences, n_antennas, rng=rng)\n",
    "\n",
    "#     # Initialize an empty array to store the covariance matrices\n",
    "#     C = tf.zeros((batch_size, n_antennas, n_antennas), dtype=tf.complex64)\n",
    "\n",
    "#     for i in range(batch_size):\n",
    "#         # Calculate the covariance matrix for each sample\n",
    "#         t_i = t[i, :]\n",
    "#         C_i = tf.linalg.toeplitz(t_i)\n",
    "#         C = tf.tensor_scatter_nd_add(C, [[i]], tf.expand_dims(C_i, axis=0))\n",
    "\n",
    "#     print('Generated ' + str(batch_size) + ' SIMO channel samples of size ' + str(n_antennas) + 'x1.')\n",
    "\n",
    "#     return h, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:53:08.759514: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-15 11:53:08.798242: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-15 11:53:08.799117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-15 11:53:12.642428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class cond_normal_channel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __call__(self, x, no, batch_size, n_coherence, n_antennas, h=None, C=None):\n",
    "        if h is None or C is None:\n",
    "            h, C = channel_generation(batch_size, n_coherence, n_antennas)\n",
    "\n",
    "        # Noise generation\n",
    "        real_noise = tf.random.normal(shape=h.shape, mean=0.0, stddev=tf.sqrt(no/2))\n",
    "        imag_noise = tf.random.normal(shape=h.shape, mean=0.0, stddev=tf.sqrt(no/2))\n",
    "        complex_noise = tf.complex(real_noise, imag_noise)\n",
    "\n",
    "        x = tf.reshape(x, [-1, 1, 1])\n",
    "\n",
    "        y = h * x + complex_noise\n",
    "\n",
    "        # Uncomment to print first 10 elements of y\n",
    "        # print('first 10 elements of y: ', y[0,0,:10])\n",
    "\n",
    "        return y, h, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class equalizer(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, h_hat, y, no):\n",
    "               \n",
    "        norm_h_hat_squared = tf.reduce_sum(tf.square(tf.abs(h_hat)), axis=-1)\n",
    "                        \n",
    "        no_new = tf.math.divide_no_nan(\n",
    "            no,\n",
    "            norm_h_hat_squared\n",
    "        )\n",
    "                        \n",
    "        inner_product_h_y = tf.reduce_sum(tf.matmul(y, tf.linalg.adjoint(h_hat)), axis=-1)\n",
    "                                                \n",
    "        x_hat = tf.math.divide_no_nan(\n",
    "            inner_product_h_y,\n",
    "            tf.cast(norm_h_hat_squared, dtype=tf.complex64)\n",
    "        )\n",
    "                        \n",
    "        return x_hat, no_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class genie_mmse_estimator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __call__(self, y, no, C, pilot):\n",
    "        \n",
    "        # print('no: ', no)\n",
    "                                \n",
    "        # noise_var = no^2 * I. Be careful of the data types!\n",
    "        noise_var = tf.cast(no * tf.eye(C.shape[-1], batch_shape=[C.shape[0]]), dtype=tf.complex64)\n",
    "                                        \n",
    "        # scaled_C = |p|^2 * C\n",
    "        scaled_C = tf.cast(tf.math.abs(pilot) ** 2, dtype=tf.complex64) * tf.cast(C, dtype=tf.complex64)\n",
    "                \n",
    "        eigenvalues, eigenvectors = tf.linalg.eigh(scaled_C)\n",
    "                        \n",
    "        # Compute the inverse of (Lambda * noise_var)^-1\n",
    "        inverse_lambda_noise_var = tf.linalg.inv(tf.linalg.diag(eigenvalues) + noise_var)\n",
    "\n",
    "        # Compute the inverse of the sum\n",
    "        inverse = tf.matmul(tf.matmul(eigenvectors, inverse_lambda_noise_var), tf.transpose(eigenvectors, conjugate=True, perm=[0, 2, 1]))\n",
    "        \n",
    "        # inverse = tf.linalg.inv(scaled_C + noise_var)\n",
    "                        \n",
    "        # scaled_C_2 = conj(p) * C\n",
    "        scaled_C_2 = tf.math.conj(pilot) * tf.cast(C, dtype=tf.complex64)                        \n",
    "\n",
    "        # matrix = scaled_C_2 * inverse\n",
    "        matrix = tf.matmul(scaled_C_2, inverse)\n",
    "\n",
    "                                                                                                                            \n",
    "        # h_hat_mmse = (scaled_C_2 * inverse) * y. Be careful of the data types!\n",
    "        h_hat_mmse = tf.matmul(matrix, tf.transpose(tf.cast(y, dtype=tf.complex64), perm=[0, 2, 1]))   \n",
    "\n",
    "                                                \n",
    "        return tf.transpose(h_hat_mmse, perm=[0, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ls_estimator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, y, x):\n",
    "        h_hat_ls = tf.math.divide_no_nan(y, x)\n",
    "                \n",
    "        return h_hat_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_random_state(seed):\n",
    "    import numbers\n",
    "    \"\"\"Turn seed into a np.random.RandomState instance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : None, int or instance of RandomState\n",
    "        If seed is None, return the RandomState singleton used by np.random.\n",
    "        If seed is an int, return a new RandomState instance seeded with seed.\n",
    "        If seed is already a RandomState instance, return it.\n",
    "        Otherwise raise ValueError.\n",
    "    \"\"\"\n",
    "    if seed is None or seed is np.random:\n",
    "        return np.random.mtrand._rand\n",
    "    if isinstance(seed, numbers.Integral):\n",
    "        return np.random.RandomState(seed)\n",
    "    if isinstance(seed, np.random.RandomState):\n",
    "        return seed\n",
    "    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'\n",
    "                     ' instance' % seed)\n",
    "\n",
    "\n",
    "def cplx2real(vec: np.ndarray, axis=0):\n",
    "    \"\"\"\n",
    "    Concatenate real and imaginary parts of vec along axis=axis.\n",
    "    \"\"\"\n",
    "    return np.concatenate([vec.real, vec.imag], axis=axis)\n",
    "\n",
    "\n",
    "def multivariate_normal_cplx(mean, covariance, n_samples, covariance_type):\n",
    "    if covariance_type == 'diag':\n",
    "        cov_sqrt = np.diag(np.sqrt(covariance))\n",
    "    elif covariance_type == 'spherical':\n",
    "        cov_sqrt = np.sqrt(covariance) * np.eye(mean.shape[0])\n",
    "    else:\n",
    "        cov_sqrt = np.linalg.cholesky(covariance)\n",
    "    h = np.squeeze(cov_sqrt @ crandn(n_samples, cov_sqrt.shape[0], 1))\n",
    "    if n_samples > 1:\n",
    "        h += np.expand_dims(mean, 0)\n",
    "    return h\n",
    "\n",
    "\n",
    "def crandn(*arg, rng=np.random.default_rng()):\n",
    "    return np.sqrt(0.5) * (rng.standard_normal(arg) + 1j * rng.standard_normal(arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "from scipy import linalg as scilinalg\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.special import logsumexp\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn import cluster\n",
    "\n",
    "\n",
    "def compute_precision_cholesky(covariances, covariance_type):\n",
    "    \"\"\"Compute the Cholesky decomposition of the precisions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    covariances : array-like\n",
    "        The covariance matrix of the current components.\n",
    "        The shape depends of the covariance_type.\n",
    "\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "        The type of precision matrices.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precisions_cholesky : array-like\n",
    "        The cholesky decomposition of sample precisions of the current\n",
    "        components. The shape depends of the covariance_type.\n",
    "    \"\"\"\n",
    "    estimate_precision_error_message = (\n",
    "        \"Fitting the mixture model failed because some components have \"\n",
    "        \"ill-defined empirical covariance (for instance caused by singleton \"\n",
    "        \"or collapsed samples). Try to decrease the number of components, \"\n",
    "        \"or increase reg_covar.\")\n",
    "\n",
    "    if covariance_type == 'full':\n",
    "        n_components, n_features, _ = covariances.shape\n",
    "        precisions_chol = np.empty((n_components, n_features, n_features), dtype=complex)\n",
    "        for k, covariance in enumerate(covariances):\n",
    "            try:\n",
    "                cov_chol = scilinalg.cholesky(covariance, lower=True)\n",
    "            except scilinalg.LinAlgError:\n",
    "                raise ValueError(estimate_precision_error_message)\n",
    "            precisions_chol[k] = scilinalg.solve_triangular(cov_chol, np.eye(n_features), lower=True).T.conj()\n",
    "    else:\n",
    "        if np.any(np.less_equal(covariances, 0.0)):\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = 1. / np.sqrt(covariances).conj()\n",
    "    return precisions_chol\n",
    "\n",
    "\n",
    "def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):\n",
    "    \"\"\"Compute the log-det of the cholesky decomposition of matrices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_chol : array-like\n",
    "        Cholesky decompositions of the matrices.\n",
    "        'full' : shape of (n_components, n_features, n_features)\n",
    "        'tied' : shape of (n_features, n_features)\n",
    "        'diag' : shape of (n_components, n_features)\n",
    "        'spherical' : shape of (n_components,)\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "    n_features : int\n",
    "        Number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    log_det_precision_chol : array-like of shape (n_components,)\n",
    "        The determinant of the precision matrix for each component.\n",
    "    \"\"\"\n",
    "    if covariance_type == \"full\":\n",
    "        n_components, _, _ = matrix_chol.shape\n",
    "        log_det_chol = np.sum(\n",
    "            np.log(matrix_chol.reshape(n_components, -1)[:, :: n_features + 1]), 1\n",
    "        )\n",
    "    elif covariance_type == \"diag\":\n",
    "        log_det_chol = np.sum(np.log(matrix_chol), axis=1)\n",
    "    else:\n",
    "        log_det_chol = n_features * (np.log(matrix_chol))\n",
    "    return log_det_chol\n",
    "\n",
    "\n",
    "def create_mimo_gmm(channels_train_vec, gmm_rx, gmm_tx, n_rx, n_tx, n_components_rx, n_components_tx):\n",
    "    num_covs = n_components_rx * n_components_tx\n",
    "\n",
    "    covs_kron_cplx = np.zeros([num_covs, n_rx * n_tx, n_rx * n_tx], dtype=complex)\n",
    "    means_kron_cplx = np.zeros([num_covs, n_rx * n_tx], dtype=complex)\n",
    "    covs_rx = gmm_rx.covs_cplx.copy()\n",
    "    covs_tx = gmm_tx.covs_cplx.copy()\n",
    "    means_rx = gmm_rx.means_cplx.copy()\n",
    "    means_tx = gmm_tx.means_cplx.copy()\n",
    "    it = 0\n",
    "    for n_r in range(n_components_rx):\n",
    "        for n_t in range(n_components_tx):\n",
    "            #covs_kron_cplx[it, :, :] = kron_real(covs_tx[n_t, :, :], covs_rx[n_r, :, :])\n",
    "            covs_kron_cplx[it, :, :] = np.kron(covs_tx[n_t, :, :], covs_rx[n_r, :, :])\n",
    "            means_kron_cplx[it, :] = np.kron(means_tx[n_t, :], means_rx[n_r, :])\n",
    "            it += 1\n",
    "    chols = compute_precision_cholesky(covs_kron_cplx, 'full')\n",
    "\n",
    "    gmm_kron_cplx = Gmm(\n",
    "        n_components=n_components_rx * n_components_tx,\n",
    "        random_state=2,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        covariance_type='full',\n",
    "    )\n",
    "\n",
    "    gmm_kron_cplx.params['zero_mean'] = False\n",
    "    # initialize parameters such that weights are not None\n",
    "    gmm_kron_cplx._initialize_parameters(channels_train_vec, random_state=None)\n",
    "\n",
    "    gmm_kron_cplx.gm.precisions_cholesky_ = chols.copy()\n",
    "    gmm_kron_cplx.gm.covariances_ = covs_kron_cplx.copy()\n",
    "    gmm_kron_cplx.gm.n_features_in_ = n_rx * n_tx\n",
    "    gmm_kron_cplx.gm.means_ = means_kron_cplx.copy()\n",
    "    gmm_kron_cplx.means_cplx = gmm_kron_cplx.gm.means_.copy()\n",
    "    gmm_kron_cplx.covs_cplx = covs_kron_cplx.copy()\n",
    "\n",
    "    # compute weights by performing single e-step\n",
    "    n_samples = channels_train_vec.shape[0]\n",
    "    _, log_resp = gmm_kron_cplx._e_step(channels_train_vec)\n",
    "    resp = np.exp(log_resp)\n",
    "    weights = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    weights /= n_samples\n",
    "    gmm_kron_cplx.gm.weights_ = weights\n",
    "\n",
    "    return gmm_kron_cplx\n",
    "\n",
    "\n",
    "class Gmm():\n",
    "    def __init__(self, *gmm_args, **gmm_kwargs):\n",
    "        self.gm = GaussianMixture(*gmm_args, **gmm_kwargs)\n",
    "        self.means_cplx = None\n",
    "        self.covs_cplx = None\n",
    "        self.chol = None\n",
    "        self.params = dict()\n",
    "        self.F2 = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.gm.__repr__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.gm.__str__()\n",
    "\n",
    "    @property\n",
    "    def covariances(self):\n",
    "        return self.gm.covariances_.copy()\n",
    "\n",
    "    @property\n",
    "    def converged(self):\n",
    "        return self.gm.converged_\n",
    "\n",
    "    @property\n",
    "    def means(self):\n",
    "        return self.gm.means_.copy()\n",
    "\n",
    "    @property\n",
    "    def precisions(self):\n",
    "        return np.einsum('ijk,ilk->ijl', self.gm.precisions_cholesky_, self.gm.precisions_cholesky_.conj())\n",
    "\n",
    "    @property\n",
    "    def precisions_cholesky(self):\n",
    "        return self.gm.precisions_cholesky_.copy()\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.gm.weights_.copy()\n",
    "\n",
    "    def fit(self, h, blocks=None, zero_mean=False):\n",
    "        \"\"\"\n",
    "        Fit an sklearn Gaussian mixture model using complex data h.\n",
    "        \"\"\"\n",
    "        if zero_mean:\n",
    "            self.params['zero_mean'] = True\n",
    "        else:\n",
    "            self.params['zero_mean'] = False\n",
    "\n",
    "        if self.gm.covariance_type == 'diag':\n",
    "            dft_matrix = np.fft.fft(np.eye(h.shape[-1], dtype=complex)) / np.sqrt(h.shape[-1])\n",
    "            self.fit_cplx(np.fft.fft(h, axis=1) / np.sqrt(h.shape[-1]))\n",
    "            self.means_cplx = self.gm.means_ @ dft_matrix.conj()\n",
    "            self.covs_cplx = np.zeros([self.means_cplx.shape[0], self.means_cplx.shape[-1],\n",
    "                                       self.means_cplx.shape[-1]], dtype=complex)\n",
    "            for i in range(self.means_cplx.shape[0]):\n",
    "                self.covs_cplx[i] = dft_matrix.conj().T @ np.diag(self.gm.covariances_[i]) @ dft_matrix\n",
    "            self.chol = compute_precision_cholesky(self.covs_cplx, 'full')\n",
    "        elif self.gm.covariance_type == 'block-diag':\n",
    "            self.gm.covariance_type = 'diag'\n",
    "            n_1, n_2 = blocks\n",
    "            F1 = np.fft.fft(np.eye(n_1)) / np.sqrt(n_1)\n",
    "            F2 = np.fft.fft(np.eye(n_2)) / np.sqrt(n_2)\n",
    "            dft_matrix = np.kron(F1, F2)\n",
    "            self.F2 = dft_matrix\n",
    "            self.fit_cplx(np.squeeze(dft_matrix @ np.expand_dims(h, 2)))\n",
    "            self.means_cplx = self.gm.means_ @ dft_matrix.conj()\n",
    "            self.covs_cplx = np.zeros([self.means_cplx.shape[0], self.means_cplx.shape[-1],\n",
    "                                       self.means_cplx.shape[-1]], dtype=complex)\n",
    "            for i in range(self.means_cplx.shape[0]):\n",
    "                self.covs_cplx[i] = dft_matrix.conj().T @ np.diag(self.gm.covariances_[i]) @ dft_matrix\n",
    "            self.chol = compute_precision_cholesky(self.covs_cplx, 'full')\n",
    "            self.gm.covariance_type = 'full'\n",
    "            self.gm.means_ = self.means_cplx\n",
    "            self.gm.precisions_cholesky_ = self.chol\n",
    "        elif self.gm.covariance_type == 'full':\n",
    "            self.fit_cplx(h)\n",
    "            self.means_cplx = self.gm.means_.copy()\n",
    "            self.covs_cplx = self.gm.covariances_.copy()\n",
    "            self.chol = self.gm.precisions_cholesky_.copy()\n",
    "        elif self.gm.covariance_type == 'toeplitz':\n",
    "            self.params['inv-em'] = True\n",
    "            self.gm.covariance_type = 'full'\n",
    "            n_1 = h.shape[1]\n",
    "            self.F2 = np.fft.fft(np.eye(2 * n_1))[:, :n_1] / np.sqrt(2 * n_1)\n",
    "            self.fit_cplx(h)\n",
    "            self.means_cplx = self.gm.means_.copy()\n",
    "            self.covs_cplx = self.gm.covariances_.copy()\n",
    "            self.chol = self.gm.precisions_cholesky_.copy()\n",
    "        elif self.gm.covariance_type == 'block-toeplitz':\n",
    "            self.params['inv-em'] = True\n",
    "            self.gm.covariance_type = 'full'\n",
    "            n_1, n_2 = blocks\n",
    "            F2_1 = np.fft.fft(np.eye(2 * n_1))[:, :n_1] / np.sqrt(2 * n_1)\n",
    "            F2_2 = np.fft.fft(np.eye(2 * n_2))[:, :n_2] / np.sqrt(2 * n_2)\n",
    "            self.F2 = np.kron(F2_1, F2_2)\n",
    "            self.fit_cplx(h)\n",
    "            self.means_cplx = self.gm.means_.copy()\n",
    "            self.covs_cplx = self.gm.covariances_.copy()\n",
    "            self.chol = self.gm.precisions_cholesky_.copy()\n",
    "        elif self.gm.covariance_type == 'circulant':\n",
    "            self.params['inv-em'] = True\n",
    "            self.gm.covariance_type = 'full'\n",
    "            n_dim = h.shape[1]\n",
    "            self.F2 = np.fft.fft(np.eye(n_dim)) / np.sqrt(n_dim)\n",
    "            self.fit_cplx(h)\n",
    "            self.means_cplx = self.gm.means_.copy()\n",
    "            self.covs_cplx = self.gm.covariances_.copy()\n",
    "            self.chol = self.gm.precisions_cholesky_.copy()\n",
    "        elif self.gm.covariance_type == 'block-circulant':\n",
    "            self.params['inv-em'] = True\n",
    "            self.gm.covariance_type = 'full'\n",
    "            n_1, n_2 = blocks\n",
    "            F1 = np.fft.fft(np.eye(n_1)) / np.sqrt(n_1)\n",
    "            F2 = np.fft.fft(np.eye(n_2)) / np.sqrt(n_2)\n",
    "            self.F2 = np.kron(F1, F2)\n",
    "            self.fit_cplx(h)\n",
    "            self.means_cplx = self.gm.means_.copy()\n",
    "            self.covs_cplx = self.gm.covariances_.copy()\n",
    "            self.chol = self.gm.precisions_cholesky_.copy()\n",
    "        else:\n",
    "            raise NotImplementedError(f'Fitting for covariance_type = {self.gm.covariance_type} is not implemented.')\n",
    "\n",
    "    def estimate_from_y(self, y, snr_dB, n_antennas, A=None, n_summands_or_proba=1):\n",
    "        \"\"\"\n",
    "        Use the noise covariance matrix and the matrix A to update the\n",
    "        covariance matrices of the Gaussian mixture model. This GMM is then\n",
    "        used for channel estimation from y.\n",
    "\n",
    "        Args:\n",
    "            y: A 2D complex numpy array.\n",
    "            snr_dB: The SNR in dB.\n",
    "            n_antennas: The dimension of the channels.\n",
    "            A: A complex observation matrix.\n",
    "            n_summands_or_proba:\n",
    "                If equal to 'all', compute the sum of all LMMSE estimates.\n",
    "                If equal to an integer, compute the sum of the top (highest\n",
    "                    component probabilities) n_summands_or_proba LMMSE\n",
    "                    estimates.\n",
    "                If equal to a float, compute the sum of as many LMMSE estimates\n",
    "                    as are necessary to reach at least a cumulative component\n",
    "                    probability of n_summands_or_proba.\n",
    "        \"\"\"\n",
    "\n",
    "        if A is None:\n",
    "            A = np.eye(n_antennas, dtype=y.dtype)\n",
    "        y_for_prediction, covs_Cy_inv = self._prepare_for_prediction(y, A, snr_dB)\n",
    "\n",
    "        h_est = np.zeros([y.shape[0], A.shape[-1]], dtype=complex)\n",
    "        if isinstance(n_summands_or_proba, int):\n",
    "            # n_summands_or_proba represents a number of summands\n",
    "\n",
    "            if n_summands_or_proba == 1:\n",
    "                # use predicted label to choose the channel covariance matrix\n",
    "                labels = self._predict_cplx(y_for_prediction)\n",
    "                for yi in range(y.shape[0]):\n",
    "                    mean_h = self.means_cplx[labels[yi], :]\n",
    "                    h_est[yi, :] = self._lmmse_formula(\n",
    "                        y[yi, :], mean_h, self.covs_cplx[labels[yi], :, :] @ A.conj().T, covs_Cy_inv[labels[yi], :, :], A @ mean_h)\n",
    "            else:\n",
    "                # use predicted probabilites to compute weighted sum of estimators\n",
    "                proba = self.predict_proba_cplx(y_for_prediction)\n",
    "                for yi in range(y.shape[0]):\n",
    "                    # indices for probabilites in descending order\n",
    "                    idx_sort = np.argsort(proba[yi, :])[::-1]\n",
    "                    for argproba in idx_sort[:n_summands_or_proba]:\n",
    "                        mean_h = self.means_cplx[argproba, :]\n",
    "                        h_est[yi, :] += proba[yi, argproba] * self._lmmse_formula(\n",
    "                            y[yi, :], mean_h, self.covs_cplx[argproba, :, :] @ A.conj().T, covs_Cy_inv[argproba, :, :], A @ mean_h)\n",
    "                    h_est[yi, :] /= np.sum(proba[yi, idx_sort[:n_summands_or_proba]])\n",
    "        elif n_summands_or_proba == 'all':\n",
    "            # use all predicted probabilities to compute weighted sum of estimators\n",
    "            proba = self.predict_proba_cplx(y_for_prediction)\n",
    "            for yi in range(y.shape[0]):\n",
    "                for argproba in range(proba.shape[1]):\n",
    "                    mean_h = self.means_cplx[argproba, :]\n",
    "                    h_est[yi, :] += proba[yi, argproba] * self._lmmse_formula(\n",
    "                        y[yi, :], mean_h, self.covs_cplx[argproba, :, :] @ A.conj().T, covs_Cy_inv[argproba, :, :], A @ mean_h)\n",
    "        else:\n",
    "            # n_summands_or_proba represents a probability\n",
    "            # use predicted probabilites to compute weighted sum of estimators\n",
    "            proba = self.predict_proba_cplx(y_for_prediction)\n",
    "            for yi in range(y.shape[0]):\n",
    "                # probabilities and corresponding indices in descending order\n",
    "                idx_sort = np.argsort(proba[yi, :])[::-1]\n",
    "                nr_proba = np.searchsorted(np.cumsum(proba[yi, idx_sort]), n_summands_or_proba) + 1\n",
    "                for argproba in idx_sort[:nr_proba]:\n",
    "                    mean_h = self.means_cplx[argproba, :]\n",
    "                    h_est[yi, :] += proba[yi, argproba] * self._lmmse_formula(\n",
    "                        y[yi, :], mean_h, self.covs_cplx[argproba, :, :] @ A.conj().T, covs_Cy_inv[argproba, :, :], A @ mean_h)\n",
    "                h_est[yi, :] /= np.sum(proba[yi, idx_sort[:nr_proba]])\n",
    "\n",
    "        return h_est\n",
    "\n",
    "    def _prepare_for_prediction(self, y, A, snr_dB):\n",
    "        \"\"\"\n",
    "        Replace the GMM's means and covariance matrices by the means and\n",
    "        covariance matrices of the observation. Further, in case of diagonal\n",
    "        matrices, FFT-transform the observation.\n",
    "        \"\"\"\n",
    "        sigma2 = 10 ** (-snr_dB / 10)\n",
    "\n",
    "        if self.gm.covariance_type == 'diag':\n",
    "            # raise error if A is not identity or quadratic matrix\n",
    "            try:\n",
    "                diff = np.sum(np.abs(A - np.eye(A.shape[0])) ** 2)\n",
    "                if diff > 1e-12:\n",
    "                    NotImplementedError(f'Estimation for covariance_type = {self.gm.covariance_type} with arbitrary matrix A is not implemented.')\n",
    "            except:\n",
    "                raise NotImplementedError(f'Estimation for covariance_type = {self.gm.covariance_type} with arbitrary matrix A is not implemented.')\n",
    "\n",
    "            # update GMM covs\n",
    "            covs_gm = self.gm.covariances_.copy()\n",
    "            sigma2_diag = sigma2 * np.ones(covs_gm.shape[-1])\n",
    "            for i in range(covs_gm.shape[0]):\n",
    "                covs_gm[i, :] = covs_gm[i, :] + sigma2_diag\n",
    "            self.gm.covariances_ = covs_gm.copy()      # this has no effect\n",
    "            self.gm.precisions_cholesky_ = compute_precision_cholesky(covs_gm, covariance_type='diag')\n",
    "\n",
    "            # FFT of observation\n",
    "            if 'block-diag' in self.params:\n",
    "                y_for_prediction = np.squeeze(self.F2 @ np.expand_dims(y, 2))\n",
    "            else:\n",
    "                y_for_prediction = np.fft.fft(y, axis=1) / np.sqrt(y.shape[-1])\n",
    "        elif self.gm.covariance_type == 'full':\n",
    "            # update GMM means\n",
    "            Am = np.squeeze(np.matmul(A, np.expand_dims(self.means_cplx, axis=2)))\n",
    "            # handle the case of only one GMM component\n",
    "            if Am.ndim == 1:\n",
    "                self.gm.means_ = Am[None, :]\n",
    "            else:\n",
    "                self.gm.means_ = Am\n",
    "\n",
    "            # update GMM covs\n",
    "            covs_gm = self.covs_cplx.copy()\n",
    "            covs_gm = np.matmul(np.matmul(A, covs_gm), A.conj().T)\n",
    "            sigma2_diag = sigma2 * np.eye(covs_gm.shape[-1])\n",
    "            for i in range(covs_gm.shape[0]):\n",
    "                covs_gm[i, :, :] = covs_gm[i, :, :] + sigma2_diag\n",
    "            self.gm.covariances_ = covs_gm.copy()      # this has no effect\n",
    "            self.gm.precisions_cholesky_ = compute_precision_cholesky(covs_gm, covariance_type='full')\n",
    "\n",
    "            # update GMM feature number\n",
    "            self.gm.n_features_in_ = A.shape[0]\n",
    "\n",
    "            y_for_prediction = y\n",
    "        else:\n",
    "            raise NotImplementedError(f'Estimation for covariance_type = {self.gm.covariance_type} is not implemented.')\n",
    "\n",
    "        # precompute the inverse matrices\n",
    "        cov_noise = sigma2 * np.eye(y.shape[1], dtype=complex)\n",
    "        covs_Cy_inv = np.zeros([self.covs_cplx.shape[0], A.shape[0], A.shape[0]], dtype=complex)\n",
    "        for i in range(self.covs_cplx.shape[0]):\n",
    "            covs_Cy_inv[i, :, :] = np.linalg.pinv(A @ self.covs_cplx[i, :, :] @ A.conj().T + cov_noise)\n",
    "\n",
    "        return y_for_prediction, covs_Cy_inv\n",
    "\n",
    "    def _lmmse_formula(self, y, mean_h, cov_h, cov_y_inv, mean_y):\n",
    "        return mean_h + cov_h @ (cov_y_inv @ (y - mean_y))\n",
    "\n",
    "\n",
    "    def _predict_cplx(self, X):\n",
    "        \"\"\"Predict the labels for the data samples in X using trained model.\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                X : array-like of shape (n_samples, n_features)\n",
    "                    List of n_features-dimensional data points. Each row\n",
    "                    corresponds to a single data point.\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                labels : array, shape (n_samples,)\n",
    "                    Component labels.\n",
    "                \"\"\"\n",
    "        return self._estimate_weighted_log_prob(X).argmax(axis=1)\n",
    "\n",
    "    def predict_proba_cplx(self, X):\n",
    "        \"\"\"Predict posterior probability of each component given the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            List of n_features-dimensional data points. Each row\n",
    "            corresponds to a single data point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        resp : array, shape (n_samples, n_components)\n",
    "            Returns the probability each Gaussian (state) in\n",
    "            the model given each sample.\n",
    "        \"\"\"\n",
    "        _, log_resp = self._estimate_log_prob_resp(X)\n",
    "        return np.exp(log_resp)\n",
    "\n",
    "    def _estimate_weighted_log_prob(self, X):\n",
    "        \"\"\"Estimate the weighted log-probabilities, log P(X | Z) + log weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weighted_log_prob : array, shape (n_samples, n_component)\n",
    "        \"\"\"\n",
    "        return self._estimate_log_prob(X) + self._estimate_log_weights()\n",
    "\n",
    "    def _estimate_log_weights(self):\n",
    "        return np.log(self.gm.weights_)\n",
    "\n",
    "    def _estimate_log_prob(self, X):\n",
    "        return self._estimate_log_gaussian_prob(X, self.gm.means_, self.gm.precisions_cholesky_,  self.gm.covariance_type)\n",
    "\n",
    "    def _estimate_log_gaussian_prob(self, X, means, precisions_chol, covariance_type):\n",
    "        \"\"\"Estimate the log Gaussian probability.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        means : array-like of shape (n_components, n_features)\n",
    "        precisions_chol : array-like\n",
    "            Cholesky decompositions of the precision matrices.\n",
    "            'full' : shape of (n_components, n_features, n_features)\n",
    "            'tied' : shape of (n_features, n_features)\n",
    "            'diag' : shape of (n_components, n_features)\n",
    "            'spherical' : shape of (n_components,)\n",
    "        covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "        Returns\n",
    "        -------\n",
    "        log_prob : array, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_components, _ = means.shape\n",
    "        # The determinant of the precision matrix from the Cholesky decomposition\n",
    "        # corresponds to the negative half of the determinant of the full precision\n",
    "        # matrix.\n",
    "        # In short: det(precision_chol) = - det(precision) / 2\n",
    "        log_det = np.real(_compute_log_det_cholesky(precisions_chol, covariance_type, n_features))\n",
    "\n",
    "        if covariance_type == \"full\":\n",
    "            log_prob = np.empty((n_samples, n_components))\n",
    "            for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):\n",
    "                y = np.dot(X, prec_chol.conj()) - np.dot(mu, prec_chol.conj())\n",
    "                log_prob[:, k] = np.sum(np.abs(y)**2, axis=1)\n",
    "\n",
    "        elif covariance_type == \"diag\":\n",
    "            precisions = np.abs(precisions_chol) ** 2\n",
    "            log_prob = (\n",
    "                    np.sum((np.abs(means) ** 2 * precisions), 1)\n",
    "                    - 2.0 * np.real(np.dot(X, (means.conj() * precisions).T))\n",
    "                    + np.dot(np.abs(X) ** 2, precisions.T)\n",
    "            )\n",
    "        # Since we are using the precision of the Cholesky decomposition,\n",
    "        # `- log_det_precision` becomes `+ 2 * log_det_precision_chol`\n",
    "        return -(n_features * np.log(np.pi) + log_prob) + 2*log_det\n",
    "\n",
    "    def fit_cplx(self, X, y=None):\n",
    "        \"\"\"Estimate model parameters with the EM algorithm.\n",
    "\n",
    "        The method fits the model ``n_init`` times and sets the parameters with\n",
    "        which the model has the largest likelihood or lower bound. Within each\n",
    "        trial, the method iterates between E-step and M-step for ``max_iter``\n",
    "        times until the change of likelihood or lower bound is less than\n",
    "        ``tol``, otherwise, a ``ConvergenceWarning`` is raised.\n",
    "        If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single\n",
    "        initialization is performed upon the first call. Upon consecutive\n",
    "        calls, training starts where it left off.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            List of n_features-dimensional data points. Each row\n",
    "            corresponds to a single data point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.fit_predict(X, y)\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X, y=None):\n",
    "        \"\"\"Estimate model parameters using X and predict the labels for X.\n",
    "\n",
    "        The method fits the model n_init times and sets the parameters with\n",
    "        which the model has the largest likelihood or lower bound. Within each\n",
    "        trial, the method iterates between E-step and M-step for `max_iter`\n",
    "        times until the change of likelihood or lower bound is less than\n",
    "        `tol`, otherwise, a :class:`~sklearn.exceptions.ConvergenceWarning` is\n",
    "        raised. After fitting, it predicts the most probable label for the\n",
    "        input data points.\n",
    "\n",
    "        .. versionadded:: 0.20\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            List of n_features-dimensional data points. Each row\n",
    "            corresponds to a single data point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        labels : array, shape (n_samples,)\n",
    "            Component labels.\n",
    "        \"\"\"\n",
    "        #X = _check_X(X, self.n_components, ensure_min_samples=2)\n",
    "        self.gm._check_n_features(X, reset=True)\n",
    "        #self.gm._check_initial_parameters(X)\n",
    "\n",
    "        # if we enable warm_start, we will have a unique initialisation\n",
    "        do_init = not(self.gm.warm_start and hasattr(self, 'converged_'))\n",
    "        n_init = self.gm.n_init if do_init else 1\n",
    "\n",
    "        max_lower_bound = -np.infty\n",
    "        self.gm.converged_ = False\n",
    "\n",
    "        random_state = check_random_state(self.gm.random_state)\n",
    "\n",
    "        n_samples, _ = X.shape\n",
    "        for init in range(n_init):\n",
    "            self.gm._print_verbose_msg_init_beg(init)\n",
    "\n",
    "            if do_init:\n",
    "                self._initialize_parameters(X, random_state)\n",
    "\n",
    "            lower_bound = (-np.infty if do_init else self.gm.lower_bound_)\n",
    "\n",
    "            for n_iter in range(1, self.gm.max_iter + 1):\n",
    "                prev_lower_bound = lower_bound\n",
    "\n",
    "                log_prob_norm, log_resp = self._e_step(X)\n",
    "                self._m_step(X, log_resp)\n",
    "                lower_bound = self.gm._compute_lower_bound(\n",
    "                    log_resp, log_prob_norm)\n",
    "\n",
    "                change = lower_bound - prev_lower_bound\n",
    "                self.gm._print_verbose_msg_iter_end(n_iter, change)\n",
    "\n",
    "                if abs(change) < self.gm.tol:\n",
    "                    self.gm.converged_ = True\n",
    "                    break\n",
    "\n",
    "            self.gm._print_verbose_msg_init_end(lower_bound)\n",
    "\n",
    "            if lower_bound > max_lower_bound:\n",
    "                max_lower_bound = lower_bound\n",
    "                best_params = self.gm._get_parameters()\n",
    "                best_n_iter = n_iter\n",
    "\n",
    "        if not self.gm.converged_:\n",
    "            warnings.warn('Initialization %d did not converge. '\n",
    "                          'Try different init parameters, '\n",
    "                          'or increase max_iter, tol '\n",
    "                          'or check for degenerate data.'\n",
    "                          % (init + 1), ConvergenceWarning)\n",
    "\n",
    "        self._set_parameters(best_params)\n",
    "        self.gm.n_iter_ = best_n_iter\n",
    "        self.gm.lower_bound_ = max_lower_bound\n",
    "\n",
    "        # Always do a final e-step to guarantee that the labels returned by\n",
    "        # fit_predict(X) are always consistent with fit(X).predict(X)\n",
    "        # for any value of max_iter and tol (and any random_state).\n",
    "        _, log_resp = self._e_step(X)\n",
    "\n",
    "        return log_resp.argmax(axis=1)\n",
    "\n",
    "\n",
    "    def _initialize_parameters(self, X, random_state):\n",
    "        \"\"\"Initialize the model parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape  (n_samples, n_features)\n",
    "\n",
    "        random_state : RandomState\n",
    "            A random number generator instance that controls the random seed\n",
    "            used for the method chosen to initialize the parameters.\n",
    "        \"\"\"\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        if self.gm.init_params == 'kmeans':\n",
    "            resp = np.zeros((n_samples, self.gm.n_components))\n",
    "            X_real = cplx2real(X, axis=1)\n",
    "            label = cluster.KMeans(n_clusters=self.gm.n_components, n_init=1,\n",
    "                                   random_state=random_state).fit(X_real).labels_\n",
    "            resp[np.arange(n_samples), label] = 1\n",
    "        elif self.gm.init_params == 'random':\n",
    "            resp = random_state.rand(n_samples, self.gm.n_components)\n",
    "            resp /= resp.sum(axis=1)[:, np.newaxis]\n",
    "        else:\n",
    "            raise ValueError(\"Unimplemented initialization method '%s'\"\n",
    "                             % self.gm.init_params)\n",
    "        self._initialize(X, resp)\n",
    "\n",
    "    def _initialize(self, X, resp):\n",
    "        \"\"\"Initialization of the Gaussian mixture parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        resp : array-like of shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        weights, means, covariances = self.estimate_gaussian_parameters(\n",
    "            X, resp, self.gm.reg_covar, self.gm.covariance_type)\n",
    "        weights /= n_samples\n",
    "\n",
    "        self.gm.weights_ = (weights if self.gm.weights_init is None\n",
    "                         else self.gm.weights_init)\n",
    "        self.gm.means_ = means if self.gm.means_init is None else self.gm.means_init\n",
    "\n",
    "        if self.gm.precisions_init is None:\n",
    "            self.gm.covariances_ = covariances\n",
    "            self.gm.precisions_cholesky_ = compute_precision_cholesky(\n",
    "                covariances, self.gm.covariance_type)\n",
    "            if 'inv-em' in self.params:\n",
    "                self.gm.Sigma = np.zeros([self.gm.n_components, self.F2.shape[0]])\n",
    "                for k in range(self.gm.n_components):\n",
    "                    self.gm.Sigma[k] = np.real(np.diag(self.F2 @ covariances[k] @ self.F2.conj().T))\n",
    "                    self.gm.Sigma[k][self.gm.Sigma[k] < self.gm.reg_covar] = self.gm.reg_covar\n",
    "        elif self.gm.covariance_type == 'full':\n",
    "            self.gm.precisions_cholesky_ = np.array(\n",
    "                [scipy.linalg.cholesky(prec_init, lower=True)\n",
    "                 for prec_init in self.gm.precisions_init])\n",
    "        else:\n",
    "            self.gm.precisions_cholesky_ = np.sqrt(self.gm.precisions_init)\n",
    "\n",
    "\n",
    "    def _e_step(self, X):\n",
    "        \"\"\"E step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_prob_norm : float\n",
    "            Mean of the logarithms of the probabilities of each sample in X\n",
    "\n",
    "        log_responsibility : array, shape (n_samples, n_components)\n",
    "            Logarithm of the posterior probabilities (or responsibilities) of\n",
    "            the point of each sample in X.\n",
    "        \"\"\"\n",
    "        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n",
    "        return np.mean(log_prob_norm), log_resp\n",
    "\n",
    "\n",
    "    def _estimate_log_prob_resp(self, X):\n",
    "        \"\"\"Estimate log probabilities and responsibilities for each sample.\n",
    "\n",
    "        Compute the log probabilities, weighted log probabilities per\n",
    "        component and responsibilities for each sample in X with respect to\n",
    "        the current state of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_prob_norm : array, shape (n_samples,)\n",
    "            log p(X)\n",
    "\n",
    "        log_responsibilities : array, shape (n_samples, n_components)\n",
    "            logarithm of the responsibilities\n",
    "        \"\"\"\n",
    "        weighted_log_prob = self._estimate_weighted_log_prob(X)\n",
    "        log_prob_norm = logsumexp(weighted_log_prob, axis=1)\n",
    "        with np.errstate(under='ignore'):\n",
    "            # ignore underflow\n",
    "            log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]\n",
    "        return log_prob_norm, log_resp\n",
    "\n",
    "\n",
    "    def _m_step(self, X, log_resp):\n",
    "        \"\"\"M step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        log_resp : array-like of shape (n_samples, n_components)\n",
    "            Logarithm of the posterior probabilities (or responsibilities) of\n",
    "            the point of each sample in X.\n",
    "        \"\"\"\n",
    "        n_samples, _ = X.shape\n",
    "        if 'inv-em' in self.params:\n",
    "            self.gm.weights_, self.gm.means_, self.gm.covariances_ = (\n",
    "                self.estimate_gaussian_parameters(X, np.exp(log_resp), self.gm.reg_covar,\n",
    "                                                'inv-em'))\n",
    "        else:\n",
    "            self.gm.weights_, self.gm.means_, self.gm.covariances_ = (\n",
    "                self.estimate_gaussian_parameters(X, np.exp(log_resp), self.gm.reg_covar,\n",
    "                                              self.gm.covariance_type))\n",
    "        self.gm.weights_ /= n_samples\n",
    "        self.gm.precisions_cholesky_ = compute_precision_cholesky(\n",
    "            self.gm.covariances_, self.gm.covariance_type)\n",
    "\n",
    "\n",
    "    def _set_parameters(self, params):\n",
    "        (self.gm.weights_, self.gm.means_, self.gm.covariances_,\n",
    "         self.gm.precisions_cholesky_) = params\n",
    "\n",
    "        # Attributes computation\n",
    "        _, n_features = self.gm.means_.shape\n",
    "\n",
    "        if self.gm.covariance_type == 'full':\n",
    "            self.gm.precisions_ = np.empty(self.gm.precisions_cholesky_.shape, dtype=complex)\n",
    "            for k, prec_chol in enumerate(self.gm.precisions_cholesky_):\n",
    "                self.gm.precisions_[k] = np.dot(prec_chol, prec_chol.T.conj())\n",
    "        else:\n",
    "            self.gm.precisions_ = np.abs(self.gm.precisions_cholesky_) ** 2\n",
    "\n",
    "\n",
    "    def estimate_gaussian_parameters(self, X, resp, reg_covar, covariance_type):\n",
    "        \"\"\"Estimate the Gaussian distribution parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data array.\n",
    "\n",
    "        resp : array-like of shape (n_samples, n_components)\n",
    "            The responsibilities for each data sample in X.\n",
    "\n",
    "        reg_covar : float\n",
    "            The regularization added to the diagonal of the covariance matrices.\n",
    "\n",
    "        covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "            The type of precision matrices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nk : array-like of shape (n_components,)\n",
    "            The numbers of data samples in the current components.\n",
    "\n",
    "        means : array-like of shape (n_components, n_features)\n",
    "            The centers of the current components.\n",
    "\n",
    "        covariances : array-like\n",
    "            The covariance matrix of the current components.\n",
    "            The shape depends of the covariance_type.\n",
    "        \"\"\"\n",
    "        nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "        means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "        if self.params['zero_mean']:\n",
    "            means = np.zeros_like(means)\n",
    "        covariances = {\"full\": self.estimate_gaussian_covariances_full,\n",
    "                       \"diag\": self.estimate_gaussian_covariances_diag,\n",
    "                       \"inv-em\": self.estimate_gaussian_covariances_inv,\n",
    "                       }[covariance_type](resp, X, nk, means, reg_covar)\n",
    "        return nk, means, covariances\n",
    "\n",
    "    def estimate_gaussian_covariances_full(self, resp, X, nk, means, reg_covar):\n",
    "        \"\"\"Estimate the full covariance matrices.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        resp : array-like of shape (n_samples, n_components)\n",
    "\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        nk : array-like of shape (n_components,)\n",
    "\n",
    "        means : array-like of shape (n_components, n_features)\n",
    "\n",
    "        reg_covar : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        covariances : array, shape (n_components, n_features, n_features)\n",
    "            The covariance matrix of the current components.\n",
    "        \"\"\"\n",
    "        n_components, n_features = means.shape\n",
    "        covariances = np.empty((n_components, n_features, n_features), dtype=complex)\n",
    "        for k in range(n_components):\n",
    "            diff = X - means[k]\n",
    "            covariances[k] = np.dot(resp[:, k] * diff.T, diff.conj()) / nk[k]\n",
    "            covariances[k].flat[::n_features + 1] += reg_covar\n",
    "        return covariances\n",
    "\n",
    "    def estimate_gaussian_covariances_diag(self, resp, X, nk, means, reg_covar):\n",
    "        \"\"\"Estimate the diagonal covariance vectors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        responsibilities : array-like of shape (n_samples, n_components)\n",
    "\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        nk : array-like of shape (n_components,)\n",
    "\n",
    "        means : array-like of shape (n_components, n_features)\n",
    "\n",
    "        reg_covar : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        covariances : array, shape (n_components, n_features)\n",
    "            The covariance vector of the current components.\n",
    "        \"\"\"\n",
    "        avg_X2 = np.dot(resp.T, X * X.conj()) / nk[:, np.newaxis]\n",
    "        avg_means2 = np.abs(means) ** 2\n",
    "        avg_X_means = means.conj() * np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "        return avg_X2 - 2.0 * np.real(avg_X_means) + avg_means2 + reg_covar\n",
    "\n",
    "    def estimate_gaussian_covariances_inv(self, resp, X, nk, means, reg_covar):\n",
    "        \"\"\"Estimate the Topelitz-structured covariance matrices.\n",
    "        Method is used from T. A. Barton and D. R. Fuhrmann, \"Covariance estimation for multidimensional data\n",
    "        using the EM algorithm,\" Proceedings of 27th Asilomar Conference on Signals, Systems and Computers, 1993,\n",
    "        pp. 203-207 vol.1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        resp : array-like of shape (n_samples, n_components)\n",
    "\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        nk : array-like of shape (n_components,)\n",
    "\n",
    "        means : array-like of shape (n_components, n_features)\n",
    "\n",
    "        reg_covar : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        covariances : array, shape (n_components, n_features, n_features)\n",
    "            The covariance matrix of the current components.\n",
    "        \"\"\"\n",
    "        n_components, n_features = means.shape\n",
    "        covariances = np.empty((n_components, n_features, n_features), dtype=complex)\n",
    "        Cinv = np.linalg.pinv(self.gm.covariances_, hermitian=True)\n",
    "        for k in range(n_components):\n",
    "            diff = X - means[k]\n",
    "            covariances[k] = np.dot(resp[:, k] * diff.T, diff.conj()) / nk[k]\n",
    "            Theta = np.real(self.F2 @ (Cinv[k] @ covariances[k] @ Cinv[k] - Cinv[k]) @ self.F2.conj().T)\n",
    "            self.gm.Sigma[k] = self.gm.Sigma[k] + np.diag(np.multiply(np.multiply(self.gm.Sigma[k], Theta), self.gm.Sigma[k]))\n",
    "            self.gm.Sigma[k][self.gm.Sigma[k] < reg_covar] = reg_covar\n",
    "            covariances[k] = np.multiply(self.F2.conj().T, self.gm.Sigma[k]) @ self.F2\n",
    "            covariances[k].flat[::n_features + 1] += reg_covar\n",
    "        return covariances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class training_samples:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def __call__(self, training_batch_size, n_coherence, n_antennas):\n",
    "        # Filename based on parameters\n",
    "        filename = f\"training_samples_{training_batch_size}x{n_coherence}x{n_antennas}.csv\"\n",
    "        filepath = os.path.join(\"training_csv_files\", filename)\n",
    "\n",
    "        # Check if file exists\n",
    "        if os.path.exists(filepath):\n",
    "            # Read from CSV\n",
    "            df = pd.read_csv(filepath, header=None)\n",
    "            h = df.to_numpy()\n",
    "\n",
    "            # Convert string representation of complex numbers to actual complex numbers\n",
    "            h = np.array([[self.convert_to_complex(num) for num in row] for row in h], dtype=np.complex64)\n",
    "\n",
    "            # Reshape to 3D array\n",
    "            print(f\"Data read from {filepath}\")\n",
    "        else:\n",
    "            # If file doesn't exist, generate, save, and return h\n",
    "            h, _ = channel_generation(training_batch_size, n_coherence, n_antennas)\n",
    "\n",
    "            print('shape of h: ', h.shape)\n",
    "\n",
    "            self.save_h_to_csv(h, filename)\n",
    "            h = np.squeeze(h, axis=1)\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def save_h_to_csv(self, h, filename):\n",
    "        # Create directory if it doesn't exist\n",
    "        directory = os.path.join(\"training_csv_files\")\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        # Path for the CSV file\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        # Formatting and saving\n",
    "        formatted_h = np.array([[\"{0.real}+{0.imag}j\".format(num) for num in row.flatten()] for row in h])\n",
    "        df = pd.DataFrame(formatted_h)\n",
    "        df.to_csv(filepath, index=False, header=False)\n",
    "        print(f\"Data saved to {filepath}\")\n",
    "    \n",
    "    def convert_to_complex(self, num_str):\n",
    "        \"\"\"\n",
    "        Convert a string representation of a complex number to a complex number.\n",
    "        Handles both 'real+imagj' and 'real-imagj' formats.\n",
    "        \"\"\"\n",
    "        if 'j' not in num_str:\n",
    "            return complex(float(num_str), 0.0)\n",
    "        real_part, imag_part = num_str.split('j')[0].split('+') if '+' in num_str else num_str.split('j')[0].split('-')\n",
    "        imag_part = imag_part + '1' if imag_part == '' else imag_part\n",
    "        return complex(float(real_part), float(imag_part) * (1 if '+' in num_str else -1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:53:23.227631: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import sionna as sn\n",
    "except AttributeError:\n",
    "    import sionna as sn\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "class end2endModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_bits_per_symbol, block_length, n_coherence, n_antennas, estimator, training_batch_size=None, covariance_type=None, n_gmm_components=None, gmm_max_iterations=500, code_rate=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.code_rate = code_rate\n",
    "        \n",
    "        self.n_coherence = n_coherence\n",
    "        self.n_antennas = n_antennas\n",
    "        self.num_bits_per_symbol = num_bits_per_symbol\n",
    "        self.block_length = block_length\n",
    "        self.constellation = sn.mapping.Constellation(\"qam\", self.num_bits_per_symbol)\n",
    "        if self.code_rate != 0:\n",
    "            self.encoder = sn.fec.ldpc.LDPC5GEncoder(self.block_length - self.num_bits_per_symbol, int((self.block_length - self.num_bits_per_symbol) / self.code_rate))\n",
    "            self.decoder = sn.fec.ldpc.LDPC5GDecoder(self.encoder, hard_out=False)\n",
    "        self.mapper = sn.mapping.Mapper(constellation=self.constellation)\n",
    "        self.demapper = sn.mapping.Demapper(\"app\", constellation=self.constellation)\n",
    "        self.binary_source = sn.utils.BinarySource()\n",
    "        self.channel = cond_normal_channel()\n",
    "        \n",
    "        self.ls_estimator = ls_estimator()\n",
    "        self.mmse_estimator = genie_mmse_estimator()\n",
    "\n",
    "        if self.estimator == 'gmm':\n",
    "            model_dir = '../training_samples_gmm/training_models'\n",
    "            os.makedirs(model_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "            # Define the full path for the model file\n",
    "            model_filename = os.path.join(model_dir, f'gmm_model_{n_gmm_components}x{training_batch_size}x{covariance_type}.pkl')\n",
    "\n",
    "\n",
    "            if os.path.exists(model_filename):\n",
    "                print(f\"loading GMM model from {model_filename}\")\n",
    "                with open(model_filename, 'rb') as file:\n",
    "                    self.gmm_estimator = pickle.load(file)\n",
    "            else:\n",
    "                self.gmm_estimator = Gmm(\n",
    "                    n_components = n_gmm_components,\n",
    "                    random_state = 2,\n",
    "                    max_iter = gmm_max_iterations,\n",
    "                    verbose = 2,\n",
    "                    n_init = 1,\n",
    "                    covariance_type = covariance_type,\n",
    "                )\n",
    "\n",
    "                self.training_gmm = training_samples()\n",
    "\n",
    "                tic = time.time()\n",
    "\n",
    "                h_training = self.training_gmm(training_batch_size, n_coherence, n_antennas)\n",
    "\n",
    "                self.gmm_estimator.fit(h_training)\n",
    "\n",
    "                toc = time.time()\n",
    "\n",
    "                print(f\"training done. ({toc - tic:.3f} s)\")\n",
    "\n",
    "                with open(model_filename, 'wb') as file:\n",
    "                    pickle.dump(self.gmm_estimator, file)\n",
    "     \n",
    "        \n",
    "        self.equalizer = equalizer()\n",
    "        \n",
    "        self.demapper = sn.mapping.Demapper(\"app\", constellation=self.constellation, num_bits_per_symbol=self.num_bits_per_symbol)\n",
    "\n",
    "    def __call__(self, batch_size, ebno_db):\n",
    "        \n",
    "        #pilot phase\n",
    "        \n",
    "        no = sn.utils.ebnodb2no(ebno_db,\n",
    "                                num_bits_per_symbol=self.num_bits_per_symbol,\n",
    "                                coderate=self.code_rate)\n",
    "        \n",
    "        \n",
    "        #print('value of no: ', no)\n",
    "\n",
    "        bits = self.binary_source([batch_size, self.block_length]) # Blocklength set to 1024 bits\n",
    "        bits = bits[:, self.num_bits_per_symbol:]\n",
    "\n",
    "        if self.code_rate != 0:\n",
    "            coded_bits = self.encoder(bits)\n",
    "            x = self.mapper(coded_bits)\n",
    "        else:\n",
    "            x = self.mapper(bits)\n",
    "                                        \n",
    "        pilot = tf.ones([batch_size,1,1], dtype=tf.complex64)\n",
    "                        \n",
    "        y_p, h, C = self.channel(pilot, no, batch_size, self.n_coherence, self.n_antennas)\n",
    "\n",
    "        if self.estimator == 'ls':\n",
    "            h_hat_ls = self.ls_estimator(y_p, pilot)\n",
    "\n",
    "            y = []\n",
    "            x_hat_ls = []\n",
    "            no_ls_new = []\n",
    "\n",
    "            if self.code_rate != 0:\n",
    "                llr_ls = tf.TensorArray(dtype=tf.float32, size=tf.cast(self.block_length - self.num_bits_per_symbol, dtype=tf.int32))\n",
    "\n",
    "                for i in range(tf.shape(x)[1]):\n",
    "                    y_i = self.channel(x[:, i], no, batch_size, self.n_coherence, self.n_antennas, h, C)[0]\n",
    "                    y.append(y_i)\n",
    "                \n",
    "                    x_hat_ls_i, no_ls_new_i = self.equalizer(h_hat_ls, y_i, no)\n",
    "                    x_hat_ls.append(x_hat_ls_i)\n",
    "                    no_ls_new.append(no_ls_new_i)\n",
    "                    \n",
    "\n",
    "                    llr_ls_i = self.demapper([x_hat_ls_i, no_ls_new_i])\n",
    "                    llr_ls = llr_ls.write(i, llr_ls_i)\n",
    "                \n",
    "                llr_ls = llr_ls.stack()\n",
    "                llr_ls = tf.transpose(llr_ls, perm=[1, 0, 2])\n",
    "\n",
    "                llr_ls = tf.split(llr_ls, num_or_size_splits=2, axis=2)\n",
    "\n",
    "                llr_ls = tf.reshape(tf.stack(llr_ls, axis=2), coded_bits.shape)\n",
    "\n",
    "                llr_ls = self.decoder(llr_ls)\n",
    "\n",
    "            else:\n",
    "                llr_ls = tf.TensorArray(dtype=tf.float32, size=tf.cast(self.block_length / self.num_bits_per_symbol - 1, dtype=tf.int32))\n",
    "\n",
    "                for i in range(tf.shape(x)[1]):\n",
    "                    y_i = self.channel(x[:, i], no, batch_size, self.n_coherence, self.n_antennas, h, C)[0]\n",
    "                    y.append(y_i)\n",
    "                \n",
    "                    x_hat_ls_i, no_ls_new_i = self.equalizer(h_hat_ls, y_i, no)\n",
    "                    x_hat_ls.append(x_hat_ls_i)\n",
    "                    no_ls_new.append(no_ls_new_i)\n",
    "                    \n",
    "\n",
    "                    llr_ls_i = self.demapper([x_hat_ls_i, no_ls_new_i])\n",
    "                    llr_ls = llr_ls.write(i, llr_ls_i)\n",
    "                \n",
    "                llr_ls = llr_ls.stack()\n",
    "                llr_ls = tf.transpose(llr_ls, perm=[1, 0, 2])\n",
    "\n",
    "                llr_ls = tf.split(llr_ls, num_or_size_splits=2, axis=2)\n",
    "\n",
    "                llr_ls = tf.reshape(tf.stack(llr_ls, axis=2), bits.shape)\n",
    "            \n",
    "            return bits, llr_ls\n",
    "        \n",
    "        elif self.estimator == 'mmse':\n",
    "            h_hat_mmse = self.mmse_estimator(y_p, no, C, pilot)\n",
    "\n",
    "            y = []\n",
    "            x_hat_mmse = []\n",
    "            no_mmse_new = []\n",
    "\n",
    "            if self.code_rate != 0:\n",
    "                llr_mmse = tf.TensorArray(dtype=tf.float32, size=tf.cast(self.block_length - self.num_bits_per_symbol, dtype=tf.int32))\n",
    "\n",
    "                for i in range(tf.shape(x)[1]):\n",
    "                    y_i = self.channel(x[:, i], no, batch_size, self.n_coherence, self.n_antennas, h, C)[0]\n",
    "                    y.append(y_i)\n",
    "                \n",
    "                    x_hat_mmse_i, no_mmse_new_i = self.equalizer(h_hat_mmse, y_i, no)\n",
    "                    x_hat_mmse.append(x_hat_mmse_i)\n",
    "                    no_mmse_new.append(no_mmse_new_i)\n",
    "                    \n",
    "\n",
    "                    llr_mmse_i = self.demapper([x_hat_mmse_i, no_mmse_new_i])\n",
    "                    llr_mmse = llr_mmse.write(i, llr_mmse_i)\n",
    "                \n",
    "                llr_mmse = llr_mmse.stack()\n",
    "                llr_mmse = tf.transpose(llr_mmse, perm=[1, 0, 2])\n",
    "\n",
    "                llr_mmse = tf.split(llr_mmse, num_or_size_splits=2, axis=2)\n",
    "\n",
    "                llr_mmse = tf.reshape(tf.stack(llr_mmse, axis=2), coded_bits.shape)\n",
    "\n",
    "                llr_mmse = self.decoder(llr_mmse)\n",
    "\n",
    "            else:\n",
    "                llr_mmse = tf.TensorArray(dtype=tf.float32, size=tf.cast(self.block_length / self.num_bits_per_symbol - 1, dtype=tf.int32))\n",
    "\n",
    "                for i in range(tf.shape(x)[1]):\n",
    "                    y_i = self.channel(x[:, i], no, batch_size, self.n_coherence, self.n_antennas, h, C)[0]\n",
    "                    y.append(y_i)\n",
    "                \n",
    "                    x_hat_mmse_i, no_mmse_new_i = self.equalizer(h_hat_mmse, y_i, no)\n",
    "                    x_hat_mmse.append(x_hat_mmse_i)\n",
    "                    no_mmse_new.append(no_mmse_new_i)\n",
    "                    \n",
    "\n",
    "                    llr_mmse_i = self.demapper([x_hat_mmse_i, no_mmse_new_i])\n",
    "                    llr_mmse = llr_mmse.write(i, llr_mmse_i)\n",
    "\n",
    "                llr_mmse = llr_mmse.stack()\n",
    "                llr_mmse = tf.transpose(llr_mmse, perm=[1, 0, 2])\n",
    "\n",
    "                llr_mmse = tf.split(llr_mmse, num_or_size_splits=2, axis=2)\n",
    "\n",
    "                llr_mmse = tf.reshape(tf.stack(llr_mmse, axis=2), bits.shape)\n",
    "\n",
    "            return bits, llr_mmse\n",
    "        \n",
    "        elif self.estimator == 'gmm':\n",
    "            y_p_np = y_p.numpy().astype(np.complex64)\n",
    "\n",
    "            y_p_np = np.squeeze(y_p_np, axis=1)\n",
    "\n",
    "            h_hat_gmm = self.gmm_estimator.estimate_from_y(y_p_np, ebno_db, self.n_antennas, n_summands_or_proba='all')\n",
    "\n",
    "            h_hat_gmm = tf.cast(h_hat_gmm, dtype=tf.complex64)\n",
    "\n",
    "            h_hat_gmm = tf.reshape(h_hat_gmm, (batch_size, 1, self.n_antennas))\n",
    "\n",
    "            y = []\n",
    "            x_hat_gmm = []\n",
    "            no_gmm_new = []\n",
    "\n",
    "            if self.code_rate != 0:\n",
    "                llr_gmm = tf.TensorArray(dtype=tf.float32, size=tf.cast(self.block_length - self.num_bits_per_symbol, dtype=tf.int32))\n",
    "\n",
    "                for i in range(tf.shape(x)[1]):\n",
    "                    y_i = self.channel(x[:, i], no, batch_size, self.n_coherence, self.n_antennas, h, C)[0]\n",
    "                    y.append(y_i)\n",
    "                \n",
    "                    x_hat_gmm_i, no_gmm_new_i = self.equalizer(h_hat_gmm, y_i, no)\n",
    "                    x_hat_gmm.append(x_hat_gmm_i)\n",
    "                    no_gmm_new.append(no_gmm_new_i)\n",
    "                    \n",
    "\n",
    "                    llr_gmm_i = self.demapper([x_hat_gmm_i, no_gmm_new_i])\n",
    "                    llr_gmm = llr_gmm.write(i, llr_gmm_i)\n",
    "                \n",
    "                llr_gmm = llr_gmm.stack()\n",
    "                llr_gmm = tf.transpose(llr_gmm, perm=[1, 0, 2])\n",
    "\n",
    "                llr_gmm = tf.split(llr_gmm, num_or_size_splits=2, axis=2)\n",
    "\n",
    "                llr_gmm = tf.reshape(tf.stack(llr_gmm, axis=2), coded_bits.shape)\n",
    "\n",
    "                llr_gmm = self.decoder(llr_gmm)\n",
    "        \n",
    "            else:\n",
    "                llr_gmm = tf.TensorArray(dtype=tf.float32, size=tf.cast(self.block_length / self.num_bits_per_symbol - 1, dtype=tf.int32))\n",
    "\n",
    "                for i in range(tf.shape(x)[1]):\n",
    "                    y_i = self.channel(x[:, i], no, batch_size, self.n_coherence, self.n_antennas, h, C)[0]\n",
    "                    y.append(y_i)\n",
    "                \n",
    "                    x_hat_gmm_i, no_gmm_new_i = self.equalizer(h_hat_gmm, y_i, no)\n",
    "                    x_hat_gmm.append(x_hat_gmm_i)\n",
    "                    no_gmm_new.append(no_gmm_new_i)\n",
    "                    \n",
    "\n",
    "                    llr_gmm_i = self.demapper([x_hat_gmm_i, no_gmm_new_i])\n",
    "                    llr_gmm = llr_gmm.write(i, llr_gmm_i)\n",
    "\n",
    "                llr_gmm = llr_gmm.stack()\n",
    "                llr_gmm = tf.transpose(llr_gmm, perm=[1, 0, 2])\n",
    "\n",
    "                llr_gmm = tf.split(llr_gmm, num_or_size_splits=2, axis=2)\n",
    "\n",
    "                llr_gmm = tf.reshape(tf.stack(llr_gmm, axis=2), bits.shape)\n",
    "            \n",
    "            return bits, llr_gmm\n",
    "        \n",
    "        else:\n",
    "            y = []\n",
    "            x_hat_real = []\n",
    "            no_real_new = []\n",
    "\n",
    "            if self.code_rate != 0:\n",
    "                llr_real = tf.TensorArray(dtype=tf.float32, size=tf.cast(self.block_length - self.num_bits_per_symbol, dtype=tf.int32))\n",
    "\n",
    "                for i in range(tf.shape(x)[1]):\n",
    "                    y_i = self.channel(x[:, i], no, batch_size, self.n_coherence, self.n_antennas, h, C)[0]\n",
    "                    y.append(y_i)\n",
    "                \n",
    "                    x_hat_real_i, no_real_new_i = self.equalizer(h, y_i, no)\n",
    "                    x_hat_real.append(x_hat_real_i)\n",
    "                    no_real_new.append(no_real_new_i)\n",
    "                    \n",
    "\n",
    "                    llr_real_i = self.demapper([x_hat_real_i, no_real_new_i])\n",
    "                    llr_real = llr_real.write(i, llr_real_i)\n",
    "                \n",
    "                llr_real = llr_real.stack()\n",
    "                llr_real = tf.transpose(llr_real, perm=[1, 0, 2])\n",
    "\n",
    "                llr_real = tf.split(llr_real, num_or_size_splits=2, axis=2)\n",
    "\n",
    "                llr_real = tf.reshape(tf.stack(llr_real, axis=2), coded_bits.shape)\n",
    "\n",
    "                llr_real = self.decoder(llr_real)\n",
    "\n",
    "            else:\n",
    "                llr_real = tf.TensorArray(dtype=tf.float32, size=tf.cast(self.block_length / self.num_bits_per_symbol - 1, dtype=tf.int32))\n",
    "\n",
    "                for i in range(tf.shape(x)[1]):\n",
    "                    y_i = self.channel(x[:, i], no, batch_size, self.n_coherence, self.n_antennas, h, C)[0]\n",
    "                    y.append(y_i)\n",
    "                \n",
    "                    x_hat_real_i, no_real_new_i = self.equalizer(h, y_i, no)\n",
    "                    x_hat_real.append(x_hat_real_i)\n",
    "                    no_real_new.append(no_real_new_i)\n",
    "                    \n",
    "\n",
    "                    llr_real_i = self.demapper([x_hat_real_i, no_real_new_i])\n",
    "                    llr_real = llr_real.write(i, llr_real_i)\n",
    "\n",
    "                llr_real = llr_real.stack()\n",
    "                llr_real = tf.transpose(llr_real, perm=[1, 0, 2])\n",
    "\n",
    "                llr_real = tf.split(llr_real, num_or_size_splits=2, axis=2)\n",
    "\n",
    "                llr_real = tf.reshape(tf.stack(llr_real, axis=2), bits.shape)\n",
    "\n",
    "            return bits, llr_real\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -10.0 | 7.5457e-02 | 2.7000e-01 |        9583 |      127000 |          135 |         500 |         4.0 |reached target block errors\n",
      "     -5.0 | 8.0797e-03 | 2.9500e-02 |        8209 |     1016000 |          118 |        4000 |        30.8 |reached target block errors\n",
      "      0.0 | 6.0970e-04 | 2.2727e-03 |        6814 |    11176000 |          100 |       44000 |       340.7 |reached target block errors\n",
      "      5.0 | 2.6535e-05 | 1.2000e-04 |         337 |    12700000 |            6 |       50000 |       387.8 |reached max iter       \n",
      "     10.0 | 0.0000e+00 | 0.0000e+00 |           0 |    12700000 |            0 |       50000 |       387.4 |reached max iter       \n",
      "\n",
      "Simulation stopped as no error occurred @ EbNo = 10.0 dB.\n",
      "\n",
      "EbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -10.0 | 2.9518e-01 | 8.5200e-01 |       37488 |      127000 |          426 |         500 |         3.7 |reached target block errors\n",
      "     -5.0 | 7.3165e-02 | 2.6600e-01 |        9292 |      127000 |          133 |         500 |         3.6 |reached target block errors\n",
      "      0.0 | 5.6874e-03 | 2.0800e-02 |        7223 |     1270000 |          104 |        5000 |        36.7 |reached target block errors\n",
      "      5.0 | 6.5038e-04 | 2.0619e-03 |        8012 |    12319000 |          100 |       48500 |       355.4 |reached target block errors\n",
      "     10.0 | 0.0000e+00 | 0.0000e+00 |           0 |    12700000 |            0 |       50000 |       367.9 |reached max iter       \n",
      "\n",
      "Simulation stopped as no error occurred @ EbNo = 10.0 dB.\n",
      "\n",
      "shape of h:  (30000, 1, 64)\n",
      "Data saved to training_csv_files/training_samples_30000x1x64.csv\n",
      "Initialization 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7f8644a97a60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/nas/ei/share/msv-users/users/ga42kab/sim-bene/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/nas/ei/share/msv-users/users/ga42kab/sim-bene/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/nas/ei/share/msv-users/users/ga42kab/sim-bene/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/nas/ei/share/msv-users/users/ga42kab/sim-bene/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 10\t time lapse 157.33308s\t ll change 1.05652\n",
      "  Iteration 20\t time lapse 150.57274s\t ll change 0.15280\n",
      "  Iteration 30\t time lapse 153.53939s\t ll change 0.07480\n",
      "  Iteration 40\t time lapse 149.90468s\t ll change 0.02684\n",
      "  Iteration 50\t time lapse 148.85649s\t ll change 0.03236\n",
      "  Iteration 60\t time lapse 149.92544s\t ll change 0.01331\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 180\u001b[0m\n\u001b[1;32m    160\u001b[0m coded_ls_model \u001b[38;5;241m=\u001b[39m end2endModel(\n\u001b[1;32m    161\u001b[0m     num_bits_per_symbol\u001b[38;5;241m=\u001b[39mnum_bits_per_symbol, \n\u001b[1;32m    162\u001b[0m     block_length\u001b[38;5;241m=\u001b[39mblock_length, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m     code_rate\u001b[38;5;241m=\u001b[39mcode_rate\n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    169\u001b[0m ber_plots\u001b[38;5;241m.\u001b[39msimulate(\n\u001b[1;32m    170\u001b[0m     coded_ls_model,\n\u001b[1;32m    171\u001b[0m     ebno_dbs\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinspace(ebno_db_min, ebno_db_max, \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     show_fig\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    178\u001b[0m )\n\u001b[0;32m--> 180\u001b[0m coded_gmm_circulant_model \u001b[38;5;241m=\u001b[39m \u001b[43mend2endModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_bits_per_symbol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bits_per_symbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_coherence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_coherence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_antennas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_antennas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcovariance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcirculant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gmm_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_gmm_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgmm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_rate\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m ber_plots\u001b[38;5;241m.\u001b[39msimulate(\n\u001b[1;32m    193\u001b[0m     coded_gmm_circulant_model,\n\u001b[1;32m    194\u001b[0m     ebno_dbs\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinspace(ebno_db_min, ebno_db_max, \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     show_fig\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    201\u001b[0m )\n\u001b[1;32m    203\u001b[0m coded_gmm_full_model \u001b[38;5;241m=\u001b[39m end2endModel(\n\u001b[1;32m    204\u001b[0m     num_bits_per_symbol\u001b[38;5;241m=\u001b[39mnum_bits_per_symbol,\n\u001b[1;32m    205\u001b[0m     block_length\u001b[38;5;241m=\u001b[39mblock_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m     code_rate\u001b[38;5;241m=\u001b[39mcode_rate\n\u001b[1;32m    213\u001b[0m )\n",
      "Cell \u001b[0;32mIn[12], line 61\u001b[0m, in \u001b[0;36mend2endModel.__init__\u001b[0;34m(self, num_bits_per_symbol, block_length, n_coherence, n_antennas, estimator, training_batch_size, covariance_type, n_gmm_components, gmm_max_iterations, code_rate)\u001b[0m\n\u001b[1;32m     57\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     59\u001b[0m h_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_gmm(training_batch_size, n_coherence, n_antennas)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgmm_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_training\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining done. (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoc\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtic\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 234\u001b[0m, in \u001b[0;36mGmm.fit\u001b[0;34m(self, h, blocks, zero_mean)\u001b[0m\n\u001b[1;32m    232\u001b[0m n_dim \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mF2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft(np\u001b[38;5;241m.\u001b[39meye(n_dim)) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(n_dim)\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_cplx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_cplx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgm\u001b[38;5;241m.\u001b[39mmeans_\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovs_cplx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgm\u001b[38;5;241m.\u001b[39mcovariances_\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[10], line 507\u001b[0m, in \u001b[0;36mGmm.fit_cplx\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_cplx\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    486\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    The method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[10], line 559\u001b[0m, in \u001b[0;36mGmm.fit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgm\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    557\u001b[0m     prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[0;32m--> 559\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_step(X, log_resp)\n\u001b[1;32m    561\u001b[0m     lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgm\u001b[38;5;241m.\u001b[39m_compute_lower_bound(\n\u001b[1;32m    562\u001b[0m         log_resp, log_prob_norm)\n",
      "Cell \u001b[0;32mIn[10], line 676\u001b[0m, in \u001b[0;36mGmm._e_step\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_e_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    661\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"E step.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_prob_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(log_prob_norm), log_resp\n",
      "Cell \u001b[0;32mIn[10], line 699\u001b[0m, in \u001b[0;36mGmm._estimate_log_prob_resp\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_log_prob_resp\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    681\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate log probabilities and responsibilities for each sample.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m    Compute the log probabilities, weighted log probabilities per\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m        logarithm of the responsibilities\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m     weighted_log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_weighted_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     log_prob_norm \u001b[38;5;241m=\u001b[39m logsumexp(weighted_log_prob, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# ignore underflow\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 435\u001b[0m, in \u001b[0;36mGmm._estimate_weighted_log_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_weighted_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    425\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate the weighted log-probabilities, log P(X | Z) + log weights.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    weighted_log_prob : array, shape (n_samples, n_component)\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_log_weights()\n",
      "Cell \u001b[0;32mIn[10], line 441\u001b[0m, in \u001b[0;36mGmm._estimate_log_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_gaussian_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeans_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecisions_cholesky_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 471\u001b[0m, in \u001b[0;36mGmm._estimate_log_gaussian_prob\u001b[0;34m(self, X, means, precisions_chol, covariance_type)\u001b[0m\n\u001b[1;32m    469\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((n_samples, n_components))\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, (mu, prec_chol) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(means, precisions_chol)):\n\u001b[0;32m--> 471\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec_chol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(mu, prec_chol\u001b[38;5;241m.\u001b[39mconj())\n\u001b[1;32m    472\u001b[0m         log_prob[:, k] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mabs(y)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m covariance_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiag\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_bits_per_symbol = 2\n",
    "block_length = 256\n",
    "ebno_db_min = -10.0 # Minimum value of Eb/N0 [dB] for simulations\n",
    "ebno_db_max = 35.0 # Maximum value of Eb/N0 [dB] for simulations\n",
    "batch_size = 500 # How many examples are processed by Sionna in parallel\n",
    "n_coherence = 1\n",
    "n_antennas = 64\n",
    "training_batch_size = 30000\n",
    "covariance_type = 'circulant'\n",
    "n_gmm_components = 128\n",
    "code_rate = 0.5\n",
    "code = 'ldpc'\n",
    "estimator = 'gmm'\n",
    "\n",
    "ber_plots = sn.utils.PlotBER(\"SC over 3GPP channel\")\n",
    "\n",
    "# uncoded_mmse_model = end2endModel(\n",
    "#     num_bits_per_symbol=num_bits_per_symbol, \n",
    "#     block_length=block_length, \n",
    "#     n_coherence=n_coherence, \n",
    "#     n_antennas=n_antennas,\n",
    "#     estimator='mmse'\n",
    "# )\n",
    "    \n",
    "# ber_plots.simulate(\n",
    "#     uncoded_mmse_model,\n",
    "#     ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "#     batch_size=batch_size,\n",
    "#     num_target_block_errors=100, # simulate until 100 block errors occured\n",
    "#     legend=\"Genie MMSE Uncoded\",\n",
    "#     soft_estimates=True,\n",
    "#     max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "#     show_fig=False   \n",
    "# )\n",
    "\n",
    "# uncoded_ls_model = end2endModel(\n",
    "#     num_bits_per_symbol=num_bits_per_symbol, \n",
    "#     block_length=block_length, \n",
    "#     n_coherence=n_coherence, \n",
    "#     n_antennas=n_antennas, \n",
    "#     estimator='ls'\n",
    "# )\n",
    "\n",
    "# ber_plots.simulate(\n",
    "#     uncoded_ls_model,\n",
    "#     ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "#     batch_size=batch_size,\n",
    "#     num_target_block_errors=100, # simulate until 100 block errors occured\n",
    "#     legend=\"LSE Uncoded\",\n",
    "#     soft_estimates=True,\n",
    "#     max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "#     show_fig=False\n",
    "# )\n",
    "\n",
    "# uncoded_gmm_circulant_model = end2endModel(\n",
    "#     num_bits_per_symbol=num_bits_per_symbol, \n",
    "#     block_length=block_length, \n",
    "#     n_coherence=n_coherence, \n",
    "#     n_antennas=n_antennas,\n",
    "#     training_batch_size=30000,\n",
    "#     covariance_type='circulant',\n",
    "#     n_gmm_components=128,\n",
    "#     estimator='gmm'\n",
    "# )\n",
    "\n",
    "# ber_plots.simulate(\n",
    "#     uncoded_gmm_circulant_model,\n",
    "#     ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "#     batch_size=batch_size,\n",
    "#     num_target_block_errors=100, # simulate until 100 block errors occured\n",
    "#     legend=\"GMM Circulant Uncoded\",\n",
    "#     soft_estimates=True,\n",
    "#     max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "#     show_fig=False\n",
    "# )\n",
    "\n",
    "# uncoded_gmm_full_model = end2endModel(\n",
    "#     num_bits_per_symbol=num_bits_per_symbol,\n",
    "#     block_length=block_length,\n",
    "#     n_coherence=n_coherence,\n",
    "#     n_antennas=n_antennas,\n",
    "#     training_batch_size=100000,\n",
    "#     covariance_type='full',\n",
    "#     n_gmm_components=128,\n",
    "#     estimator='gmm'\n",
    "# )\n",
    "\n",
    "# ber_plots.simulate(\n",
    "#     uncoded_gmm_full_model,\n",
    "#     ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "#     batch_size=batch_size,\n",
    "#     num_target_block_errors=100,\n",
    "#     legend=\"GMM Full Uncoded\",\n",
    "#     soft_estimates=True,\n",
    "#     max_mc_iter=100,\n",
    "#     show_fig=False\n",
    "# )\n",
    "\n",
    "# uncoded_sample_cov_model = end2endModel(\n",
    "#     num_bits_per_symbol=num_bits_per_symbol, \n",
    "#     block_length=block_length, \n",
    "#     n_coherence=n_coherence, \n",
    "#     n_antennas=n_antennas,\n",
    "#     training_batch_size=100000,\n",
    "#     covariance_type='full',\n",
    "#     n_gmm_components=1,\n",
    "#     estimator='gmm'\n",
    "# )\n",
    "\n",
    "# ber_plots.simulate(\n",
    "#     uncoded_sample_cov_model,\n",
    "#     ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "#     batch_size=batch_size,\n",
    "#     num_target_block_errors=100, # simulate until 100 block errors occured\n",
    "#     legend=\"Sample Covariance Uncoded\",\n",
    "#     soft_estimates=True,\n",
    "#     max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "#     show_fig=False\n",
    "# )\n",
    "\n",
    "# uncoded_real_model = end2endModel(\n",
    "#     num_bits_per_symbol=num_bits_per_symbol,\n",
    "#     block_length=block_length,\n",
    "#     n_coherence=n_coherence,\n",
    "#     n_antennas=n_antennas,\n",
    "#     estimator='real'\n",
    "# )\n",
    "\n",
    "# ber_plots.simulate(\n",
    "#     uncoded_real_model,\n",
    "#     ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "#     batch_size=batch_size,\n",
    "#     num_target_block_errors=100,\n",
    "#     legend=\"Real Uncoded\",\n",
    "#     soft_estimates=True,\n",
    "#     max_mc_iter=100,\n",
    "#     show_fig=False\n",
    "# )\n",
    "\n",
    "coded_mmse_model = end2endModel(\n",
    "    num_bits_per_symbol=num_bits_per_symbol, \n",
    "    block_length=block_length, \n",
    "    n_coherence=n_coherence, \n",
    "    n_antennas=n_antennas,\n",
    "    estimator='mmse',\n",
    "    code_rate=code_rate\n",
    ")\n",
    "\n",
    "ber_plots.simulate(\n",
    "    coded_mmse_model,\n",
    "    ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "    batch_size=batch_size,\n",
    "    num_target_block_errors=100, # simulate until 100 block errors occured\n",
    "    legend=\"Genie MMSE Coded\",\n",
    "    soft_estimates=True,\n",
    "    max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "    show_fig=False\n",
    ")\n",
    "\n",
    "coded_ls_model = end2endModel(\n",
    "    num_bits_per_symbol=num_bits_per_symbol, \n",
    "    block_length=block_length, \n",
    "    n_coherence=n_coherence, \n",
    "    n_antennas=n_antennas,\n",
    "    estimator='ls',\n",
    "    code_rate=code_rate\n",
    ")\n",
    "\n",
    "ber_plots.simulate(\n",
    "    coded_ls_model,\n",
    "    ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "    batch_size=batch_size,\n",
    "    num_target_block_errors=100, # simulate until 100 block errors occured\n",
    "    legend=\"LSE Coded\",\n",
    "    soft_estimates=True,\n",
    "    max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "    show_fig=False\n",
    ")\n",
    "\n",
    "coded_gmm_circulant_model = end2endModel(\n",
    "    num_bits_per_symbol=num_bits_per_symbol, \n",
    "    block_length=block_length, \n",
    "    n_coherence=n_coherence, \n",
    "    n_antennas=n_antennas,\n",
    "    training_batch_size=30000,\n",
    "    covariance_type='circulant',\n",
    "    n_gmm_components=n_gmm_components,\n",
    "    estimator='gmm',\n",
    "    code_rate=code_rate\n",
    ")\n",
    "\n",
    "ber_plots.simulate(\n",
    "    coded_gmm_circulant_model,\n",
    "    ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "    batch_size=batch_size,\n",
    "    num_target_block_errors=100, # simulate until 100 block errors occured\n",
    "    legend=\"GMM Circulant Coded\",\n",
    "    soft_estimates=True,\n",
    "    max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "    show_fig=False\n",
    ")\n",
    "\n",
    "coded_gmm_full_model = end2endModel(\n",
    "    num_bits_per_symbol=num_bits_per_symbol,\n",
    "    block_length=block_length,\n",
    "    n_coherence=n_coherence,\n",
    "    n_antennas=n_antennas,\n",
    "    training_batch_size=100000,\n",
    "    covariance_type='full',\n",
    "    n_gmm_components=n_gmm_components,\n",
    "    estimator='gmm',\n",
    "    code_rate=code_rate\n",
    ")\n",
    "\n",
    "ber_plots.simulate(\n",
    "    coded_gmm_full_model,\n",
    "    ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "    batch_size=batch_size,\n",
    "    num_target_block_errors=100,\n",
    "    legend=\"GMM Full Coded\",\n",
    "    soft_estimates=True,\n",
    "    max_mc_iter=100,\n",
    "    show_fig=False\n",
    ")\n",
    "\n",
    "coded_sample_cov_model = end2endModel(\n",
    "    num_bits_per_symbol=num_bits_per_symbol, \n",
    "    block_length=block_length, \n",
    "    n_coherence=n_coherence, \n",
    "    n_antennas=n_antennas,\n",
    "    training_batch_size=100000,\n",
    "    covariance_type='full',\n",
    "    n_gmm_components=1,\n",
    "    estimator='gmm',\n",
    "    code_rate=code_rate\n",
    ")\n",
    "\n",
    "ber_plots.simulate(\n",
    "    coded_sample_cov_model,\n",
    "    ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "    batch_size=batch_size,\n",
    "    num_target_block_errors=100, # simulate until 100 block errors occured\n",
    "    legend=\"Sample Covariance Coded\",\n",
    "    soft_estimates=True,\n",
    "    max_mc_iter=100, # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "    show_fig=False\n",
    ")\n",
    "\n",
    "coded_real_model = end2endModel(\n",
    "    num_bits_per_symbol=num_bits_per_symbol,\n",
    "    block_length=block_length,\n",
    "    n_coherence=n_coherence,\n",
    "    n_antennas=n_antennas,\n",
    "    estimator='real',\n",
    "    code_rate=code_rate\n",
    ")\n",
    "\n",
    "ber_plots.simulate(\n",
    "    coded_real_model,\n",
    "    ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 10),\n",
    "    batch_size=batch_size,\n",
    "    num_target_block_errors=100,\n",
    "    legend=\"Real Coded\",\n",
    "    soft_estimates=True,\n",
    "    max_mc_iter=100,\n",
    "    show_fig=False\n",
    ")\n",
    "\n",
    "simulation_data = []\n",
    "\n",
    "for i in range(len(ber_plots.legend)):\n",
    "    model_description = ber_plots.legend[i]\n",
    "    for snr, ber in zip(ber_plots.snr[i], ber_plots.ber[i]):\n",
    "        simulation_data.append({\n",
    "            'Model Description': model_description,\n",
    "            'SNR (dB)': snr,\n",
    "            'BER': ber\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the accumulated data\n",
    "df = pd.DataFrame(simulation_data)\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = f'../simulation_results/ber'  # Change to your desired path\n",
    "os.makedirs(csv_file_path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Define the full path for the model file\n",
    "sim_results_csv = os.path.join(csv_file_path, 'csv', f'BER_{n_antennas}x{n_coherence}x{batch_size}x{n_gmm_components}x{code}x{code_rate}.csv')\n",
    "\n",
    "df.to_csv(sim_results_csv, index=False)\n",
    "\n",
    "sim_results_plot = os.path.join(csv_file_path, 'plots', f'BER_{n_antennas}x{n_coherence}x{batch_size}x{n_gmm_components}x{code}x{code_rate}.png')\n",
    "\n",
    "ber_plots(\n",
    "    save_fig=True,\n",
    "    pathname=sim_results_plot\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
