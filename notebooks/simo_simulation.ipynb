{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Various helper functions.\n",
    "\n",
    "Note:\n",
    "    The functions are in alphabetical order.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def batch_kron(a: np.ndarray, b: np.ndarray):\n",
    "    \"\"\"\n",
    "    Batch Kronecker product: np.kron(a[i, :, :], b[i, :, :]) for all i.\n",
    "    \"\"\"\n",
    "    return np.einsum('bik,bjl->bijkl', a, b).reshape(a.shape[0], a.shape[1] * b.shape[1], a.shape[2] * b.shape[2])\n",
    "\n",
    "\n",
    "def cov2cov(matrices: np.ndarray):\n",
    "    \"\"\"\n",
    "    Convert the real representations of complex covariance matrices back to\n",
    "    complex representations.\n",
    "    \"\"\"\n",
    "    if matrices.ndim == 2:\n",
    "        # the case of diagonal matrices\n",
    "        n_mats, n_diag = matrices.shape\n",
    "        mats = np.zeros([n_mats, n_diag, n_diag])\n",
    "        for i in range(n_mats):\n",
    "            mats[i, :, :] = np.diag(matrices[i, :])\n",
    "    else:\n",
    "        mats = matrices\n",
    "\n",
    "    n_mats, rows, columns = mats.shape\n",
    "    row_half = rows // 2\n",
    "    column_half = columns // 2\n",
    "    covs = np.zeros((n_mats, row_half, column_half), dtype=complex)\n",
    "    for c in range(n_mats):\n",
    "        upper_left_block = mats[c, :row_half, :column_half]\n",
    "        upper_right_block = mats[c, :row_half, column_half:]\n",
    "        lower_left_block = mats[c, row_half:, :column_half]\n",
    "        lower_right_block = mats[c, row_half:, column_half:]\n",
    "        covs[c, :, :] = upper_left_block + lower_right_block + 1j * (lower_left_block - upper_right_block)\n",
    "    return covs\n",
    "\n",
    "\n",
    "def cplx2real(vec: np.ndarray, axis=0):\n",
    "    \"\"\"\n",
    "    Concatenate real and imaginary parts of vec along axis=axis.\n",
    "    \"\"\"\n",
    "    return np.concatenate([vec.real, vec.imag], axis=axis)\n",
    "\n",
    "\n",
    "def crandn(*arg, rng=np.random.default_rng()):\n",
    "    return np.sqrt(0.5) * (rng.standard_normal(arg) + 1j * rng.standard_normal(arg))\n",
    "\n",
    "\n",
    "# def crandn(*args, rng=None):\n",
    "#     # Create a TensorFlow random generator with the given seed\n",
    "#     if rng is None:\n",
    "#         tf.random.set_seed(1235428719812346)\n",
    "#         rng = tf.random.Generator.from_seed()\n",
    "    \n",
    "#     real_part = tf.sqrt(0.5) * rng.normal(shape=args)\n",
    "#     imag_part = tf.sqrt(0.5) * rng.normal(shape=args)\n",
    "\n",
    "#     print('dtype of tf.complex(real_part, imag_part): ', tf.complex(real_part, imag_part).dtype)\n",
    "    \n",
    "#     return tf.complex(real_part, imag_part)\n",
    "\n",
    "\n",
    "def kron_approx_sep_ls(\n",
    "    mats_A: np.ndarray,\n",
    "    init_C: np.ndarray,\n",
    "    rows_B: int,\n",
    "    cols_B: int,\n",
    "    iterations: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Approximate a matrix in terms of a Kronecker product of two matrices. The\n",
    "    array init_C is an initialization for the matrix C and will be overwritten.\n",
    "    If it is structured, for example, if it is positive definite, then the\n",
    "    returned matrices B and C will have the same structure. Section 5 of the\n",
    "    source explains what kind of structure can be used.\n",
    "\n",
    "    Note:\n",
    "        This corresponds to Framework 2 in Section 4 in the source.\n",
    "\n",
    "    Source:\n",
    "        \"Approximation with Kronecker Products\"\n",
    "        by Van Loan, Pitsianis\n",
    "        https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.42.1924&rep=rep1&type=pdf\n",
    "    \"\"\"\n",
    "    if mats_A.ndim == 2:\n",
    "        mats_A = np.expand_dims(mats_A, 0)\n",
    "        init_C = np.expand_dims(init_C, 0)\n",
    "\n",
    "    mats_B = np.zeros((mats_A.shape[0], rows_B, cols_B), dtype=mats_A.dtype)\n",
    "    mats_C = init_C\n",
    "    rows_C, cols_C = mats_C.shape[-2:]\n",
    "\n",
    "    # Extract the blocks A_ij: Split the 3d array of shape (n, rows_B * rows_C, cols_B * cols_C) into a 5d array of\n",
    "    # shape (n, rows_B, cols_B, rows_C, cols_C) where [:, i, j, :, :] corresponds to the blocks A_ij in equation (2).\n",
    "    blocks_A = np.zeros((mats_A.shape[0], rows_B, cols_B, rows_C, cols_C), dtype=mats_A.dtype)\n",
    "    for i, block_row in enumerate(np.split(mats_A, rows_B, axis=-2)):\n",
    "        for j, block in enumerate(np.split(block_row, cols_B, axis=-1)):\n",
    "            blocks_A[:, i, j, :, :] = block\n",
    "\n",
    "    # Extract the blocks Ahat_ij: Split the 3d array of shape (n, rows_B * rows_C, cols_B * cols_C) into a 5d array of\n",
    "    # shape (n, rows_C, cols_C, rows_B, cols_B) where [:, i, j, :, :] corresponds to the blocks Ahat_ij in equation (4).\n",
    "    blocks_Ahat = np.zeros((mats_A.shape[0], rows_C, cols_C, rows_B, cols_B), dtype=mats_A.dtype)\n",
    "    for i in range(rows_C):\n",
    "        for j in range(cols_C):\n",
    "            blocks_Ahat[:, i, j, :, :] = \\\n",
    "                mats_A[:, i: i + (rows_B - 1) * rows_C + 1: rows_C, j: j + (cols_B - 1) * cols_C + 1: cols_C]\n",
    "\n",
    "    beta_or_gamma = np.zeros((mats_A.shape[0], 1, 1), dtype=mats_A.dtype)\n",
    "\n",
    "    def project(blocks_ij, b_or_c, c_or_b_out):\n",
    "        \"\"\"\n",
    "        For every block A_ij (or Ahat_ij), compute equation (8) (or (9)) in Theorem 4.1.\n",
    "        \"\"\"\n",
    "        np.einsum('ijklm,ilm->ijk', blocks_ij, b_or_c, out=c_or_b_out)\n",
    "        np.einsum('ijk,ijk->i', b_or_c, b_or_c, out=beta_or_gamma[:, 0, 0])\n",
    "        c_or_b_out /= beta_or_gamma\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        project(blocks_A, mats_C, mats_B)\n",
    "        project(blocks_Ahat, mats_B, mats_C)\n",
    "\n",
    "    return mats_B, mats_C\n",
    "\n",
    "\n",
    "def kron_approx_svd(mats_A: np.ndarray, rows_B: int, cols_B: int, rows_C: int, cols_C: int):\n",
    "    r\"\"\"\n",
    "    Approximate a matrix in terms of a Kronecker product of two matrices.\n",
    "\n",
    "    Note:\n",
    "        Given a matrix A, find matrices B and C of shapes (rows_B, cols_B) and\n",
    "        (rows_C, cols_C) such that \\| A - B \\otimes C \\|_F is minimized.\n",
    "        If A is structured, e.g., symmetric or positive definite, the function\n",
    "        kron_approx_sep_ls can be used.\n",
    "\n",
    "    Source:\n",
    "        \"Approximation with Kronecker Products\"\n",
    "        by Van Loan, Pitsianis\n",
    "        https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.42.1924&rep=rep1&type=pdf\n",
    "    \"\"\"\n",
    "    if mats_A.ndim == 2:\n",
    "        mats_A = np.expand_dims(mats_A, 0)\n",
    "    n, rows_A, cols_A = mats_A.shape\n",
    "    if rows_B * rows_C != rows_A:\n",
    "        raise ValueError(f'rows_B*rows_C = {rows_A} is required, but rows_B*rows_C = {rows_B*rows_C}')\n",
    "    if cols_B * cols_C != cols_A:\n",
    "        raise ValueError(f'cols_B*cols_C = {cols_A} is required, but cols_B*cols_C = {cols_B*cols_C}')\n",
    "\n",
    "    def block(A, i, j):\n",
    "        \"\"\"\n",
    "        Extract block A_ij from the matrix A. Every block A_ij has shape\n",
    "        (rows_C, cols_C) and there are rows_B * cols_B such blocks.\n",
    "        \"\"\"\n",
    "        return A[i * rows_C: (i + 1) * rows_C, j * cols_C: (j + 1) * cols_C]\n",
    "\n",
    "    out_B = np.zeros((n, rows_B, cols_B), dtype=mats_A.dtype)\n",
    "    out_C = np.zeros((n, rows_C, cols_C), dtype=mats_A.dtype)\n",
    "    for ni in range(n):\n",
    "        # the following implements equation (5)\n",
    "        rearranged_A = np.zeros((rows_B * cols_B, rows_C * cols_C), dtype=mats_A.dtype)\n",
    "        for j in range(cols_B):\n",
    "            # extract the matrix A_j\n",
    "            rearranged_A[j * rows_B: (j + 1) * rows_B, :] = np.concatenate(\n",
    "                [block(mats_A[ni, :, :], i, j).flatten('F')[np.newaxis, :] for i in range(rows_B)],\n",
    "                axis=0\n",
    "            )\n",
    "        u, s, vh = np.linalg.svd(rearranged_A)\n",
    "        out_B[ni, :, :] = u[:, 0].reshape(rows_B, cols_B, order='F') * s[0]\n",
    "        out_C[ni, :, :] = vh[0, :].reshape(rows_C, cols_C, order='F')\n",
    "    return out_B, out_C\n",
    "\n",
    "\n",
    "def kron_real(mats1: np.ndarray, mats2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Assuming mats1 and mats2 are real representations of complex covariance\n",
    "    matrices, compute the real representation of the Kronecker product of the\n",
    "    complex covariance matrices.\n",
    "    \"\"\"\n",
    "    if mats1.ndim != mats2.ndim:\n",
    "        raise ValueError(\n",
    "            'The two arrays need to have the same number of dimensions, '\n",
    "            f'but we have mats1.ndim = {mats1.ndim} and mats2.ndim = {mats2.ndim}.'\n",
    "        )\n",
    "    if mats1.ndim == 2:\n",
    "        mats1 = np.expand_dims(mats1, 0)\n",
    "        mats2 = np.expand_dims(mats2, 0)\n",
    "\n",
    "    n = mats1.shape[0]\n",
    "    rows1, cols1 = mats1.shape[-2:]\n",
    "    rows2, cols2 = mats2.shape[-2:]\n",
    "    row_half1 = rows1 // 2\n",
    "    column_half1 = cols1 // 2\n",
    "    row_half2 = rows2 // 2\n",
    "    column_half2 = cols2 // 2\n",
    "    rows3 = 2 * row_half1 * row_half2\n",
    "    cols3 = 2 * column_half1 * column_half2\n",
    "\n",
    "    out_kron_prod = np.zeros((n, rows3, cols3))\n",
    "    for i in range(n):\n",
    "        A1 = mats1[i, :row_half1, :column_half1]\n",
    "        B1 = mats1[i, :row_half1, column_half1:]\n",
    "        C1 = mats1[i, row_half1:, :column_half1]\n",
    "        D1 = mats1[i, row_half1:, column_half1:]\n",
    "\n",
    "        A2 = mats2[i, :row_half2, :column_half2]\n",
    "        B2 = mats2[i, :row_half2, column_half2:]\n",
    "        C2 = mats2[i, row_half2:, :column_half2]\n",
    "        D2 = mats2[i, row_half2:, column_half2:]\n",
    "\n",
    "        A = np.kron(A1 + D1, A2 + D2)\n",
    "        D = -np.kron(C1 - B1, C2 - B2)\n",
    "        B = -np.kron(A1 + D1, C2 - B2)\n",
    "        C = np.kron(C1 - B1, A2 + D2)\n",
    "\n",
    "        A = 0.5 * (A + D)\n",
    "        D = A\n",
    "        B = 0.5 * (B - C)\n",
    "        C = -B\n",
    "\n",
    "        out_kron_prod[i, :, :] = np.concatenate(\n",
    "            (np.concatenate((A, B), axis=1), np.concatenate((C, D), axis=1)),\n",
    "            axis=0\n",
    "        )\n",
    "    return np.squeeze(out_kron_prod)\n",
    "\n",
    "\n",
    "def mat2bsc(mat: np.ndarray):\n",
    "    \"\"\"\n",
    "    Arrange the real and imaginary parts of a complex matrix mat in block-\n",
    "    skew-circulant form.\n",
    "\n",
    "    Source:\n",
    "        See https://ieeexplore.ieee.org/document/7018089.\n",
    "    \"\"\"\n",
    "    upper_half = np.concatenate((mat.real, -mat.imag), axis=-1)\n",
    "    lower_half = np.concatenate((mat.imag, mat.real), axis=-1)\n",
    "    return np.concatenate((upper_half, lower_half), axis=-2)\n",
    "\n",
    "\n",
    "def real2real(mats):\n",
    "    re = np.real(mats)\n",
    "    im = np.imag(mats)\n",
    "    rows = mats.shape[1]\n",
    "    cols = mats.shape[2]\n",
    "    out = np.zeros([mats.shape[0], 2*rows, 2*cols])\n",
    "    out[:, :rows, :cols] = 0.5*re\n",
    "    out[:, rows:, cols:] = 0.5*re\n",
    "    return out\n",
    "\n",
    "\n",
    "def imag2imag(mats):\n",
    "    im = np.real(mats)\n",
    "    rows = mats.shape[1]\n",
    "    cols = mats.shape[2]\n",
    "    out = np.zeros([mats.shape[0], 2*rows, 2*cols])\n",
    "    #out[:, :rows, :cols] = 0.5*re\n",
    "    for i in range(mats.shape[0]):\n",
    "        out[i, :rows, cols:] = 0.5*im[i,:,:].T\n",
    "        out[i, rows:, :cols] = 0.5*im[i,:,:]\n",
    "    #out[:, rows:, cols:] = 0.5*re\n",
    "    return out\n",
    "\n",
    "\n",
    "def nmse(actual: np.ndarray, desired: np.ndarray):\n",
    "    \"\"\"\n",
    "    Mean squared error between actual and desired divided by the total number\n",
    "    of elements.\n",
    "    \"\"\"\n",
    "    mse = 0\n",
    "    for i in range(actual.shape[0]):\n",
    "        mse += np.linalg.norm(actual - desired) ** 2 / np.linalg.norm(desired) ** 2\n",
    "    return mse / actual.shape[0]\n",
    "    #return np.sum(np.abs(actual - desired) ** 2) / desired.size\n",
    "\n",
    "\n",
    "def real2cplx(vec: np.ndarray, axis=0):\n",
    "    \"\"\"\n",
    "    Assume vec consists of concatenated real and imaginary parts. Return the\n",
    "    corresponding complex vector. Split along axis=axis.\n",
    "    \"\"\"\n",
    "    re, im = np.split(vec, 2, axis=axis)\n",
    "    return re + 1j * im\n",
    "\n",
    "\n",
    "def sec2hours(seconds: float):\n",
    "    \"\"\"\"\n",
    "    Convert a number of seconds to a string h:mm:ss.\n",
    "    \"\"\"\n",
    "    # hours\n",
    "    h = seconds // 3600\n",
    "    # remaining seconds\n",
    "    r = seconds % 3600\n",
    "    return '{:.0f}:{:02.0f}:{:02.0f}'.format(h, r // 60, r % 60)\n",
    "\n",
    "def check_random_state(seed):\n",
    "    import numbers\n",
    "    \"\"\"Turn seed into a np.random.RandomState instance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : None, int or instance of RandomState\n",
    "        If seed is None, return the RandomState singleton used by np.random.\n",
    "        If seed is an int, return a new RandomState instance seeded with seed.\n",
    "        If seed is already a RandomState instance, return it.\n",
    "        Otherwise raise ValueError.\n",
    "    \"\"\"\n",
    "    if seed is None or seed is np.random:\n",
    "        return np.random.mtrand._rand\n",
    "    if isinstance(seed, numbers.Integral):\n",
    "        return np.random.RandomState(seed)\n",
    "    if isinstance(seed, np.random.RandomState):\n",
    "        return seed\n",
    "    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'\n",
    "                     ' instance' % seed)\n",
    "\n",
    "def print_dict(dict: dict, entries_per_row: int=1):\n",
    "    \"\"\"Print the keys and values of dictionary dict.\"\"\"\n",
    "    if entries_per_row < 1:\n",
    "        raise ValueError(f'The number of entries per row needs to be >= 1 but is {entries_per_row}')\n",
    "    for c, (key, value) in enumerate(dict.items()):\n",
    "        if c % entries_per_row == 0 and c > 0:\n",
    "            print()\n",
    "        else:\n",
    "            c > 0 and print(' | ', end='')\n",
    "        print('{}: {}'.format(key, value), end='')\n",
    "    print()\n",
    "\n",
    "\n",
    "def dft_matrix(n_antennas, n_grid):\n",
    "    grid = np.linspace(-1, 1, n_grid + 1)[:n_grid]\n",
    "\n",
    "    d = 1 / np.sqrt(n_antennas) * np.exp(1j * np.pi * np.outer(np.arange(n_antennas), grid.conj().T))\n",
    "    return d\n",
    "\n",
    "#def dist_fac(n_bits: int):\n",
    "#    \"\"\"\"Compute distortion factor for an arbitrary number of bits.\"\"\"\n",
    "#    if n_bits == 1:\n",
    "#        raise ValueError('The distortion factor for 1-bit is known in closed form.')\n",
    "#    else:\n",
    "#        return n_bits * 2**(-2*n_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scm_channel(\n",
    "    angles,\n",
    "    weights,\n",
    "    n_coherence,\n",
    "    n_antennas,\n",
    "    sigma=2.0,\n",
    "    rng=np.random.default_rng()\n",
    "):\n",
    "    (h, t) = chan_from_spectrum(n_coherence, n_antennas, angles, weights, sigma, rng=rng)\n",
    "    return h, t\n",
    "\n",
    "\n",
    "def spectrum(u, angles, weights, sigma=2.0):\n",
    "    u = (u + np.pi) % (2 * np.pi) - np.pi\n",
    "    theta = np.degrees(np.arcsin(u / np.pi))\n",
    "    v = _laplace(theta, angles, weights, sigma) \\\n",
    "        + _laplace(180 - theta, angles, weights, sigma)\n",
    "\n",
    "    return np.degrees(2 * np.pi * v / np.sqrt(np.pi ** 2 - u ** 2))\n",
    "\n",
    "\n",
    "def _laplace(theta, angles, weights, sigma=2.0):\n",
    "    # The variance \\sigma^2 of a Laplace density is \\sigma^2 = 2 * scale_parameter^2.\n",
    "    # Hence, the standard deviation \\sigma is \\sigma = sqrt(2) * scale_parameter.\n",
    "    # The scale_parameter determines the Laplace density.\n",
    "    # For an angular spread (AS) given in terms of a standard deviation \\sigma\n",
    "    # the scale parameter thus needs to be computed as scale_parameter = \\sigma / sqrt(2)\n",
    "    scale_parameter = sigma / np.sqrt(2)\n",
    "    x_shifted = np.outer(theta, np.ones(angles.size)) - angles\n",
    "    x_shifted = (x_shifted + 180) % 360 - 180\n",
    "    v = weights / (2 * scale_parameter) * np.exp(-np.absolute(x_shifted) / scale_parameter)\n",
    "    return v.sum(axis=1)\n",
    "\n",
    "# def spectrum(u, angles, weights, sigma=2.0):\n",
    "#     pi = tf.constant(np.pi, dtype=tf.float32)\n",
    "#     u = (u + pi) % (2 * pi) - pi\n",
    "#     theta = (180.0 / pi) * tf.math.asin(u / pi)\n",
    "#     v = _laplace(theta, angles, weights, sigma) + _laplace(180 - theta, angles, weights, sigma)\n",
    "\n",
    "#     return (360.0 / pi) * (2 * pi * v / tf.sqrt(pi**2 - u**2))\n",
    "\n",
    "\n",
    "# def _laplace(theta, angles, weights, sigma=2.0):\n",
    "#     # The variance \\sigma^2 of a Laplace density is \\sigma^2 = 2 * scale_parameter^2.\n",
    "#     # Hence, the standard deviation \\sigma is \\sigma = sqrt(2) * scale_parameter.\n",
    "#     # The scale_parameter determines the Laplace density.\n",
    "#     # For an angular spread (AS) given in terms of a standard deviation \\sigma\n",
    "#     # the scale parameter thus needs to be computed as scale_parameter = \\sigma / sqrt(2)\n",
    "#     scale_parameter = sigma / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "    \n",
    "#     theta = tf.matmul(tf.reshape(theta, (theta.shape[0], 1)), tf.reshape(tf.ones_like(angles), (1, angles.shape[0])))\n",
    "\n",
    " \n",
    "#     # Calculate x_shifted using tf.matmul\n",
    "#     x_shifted = theta - tf.reshape(angles, (1, angles.shape[0]))\n",
    "\n",
    "#     x_shifted = theta - angles\n",
    "#     x_shifted = (x_shifted + 180) % 360 - 180\n",
    "#     print('x_shifted: ', x_shifted.shape)\n",
    "#     print('scale_parameter: ', scale_parameter.shape)\n",
    "#     print('weights: ', weights.shape)\n",
    "#     print('x_shifted: ', tf.exp(-tf.abs(x_shifted) / scale_parameter).shape)\n",
    "#     v = (weights / (2 * scale_parameter)) * tf.exp(-tf.abs(x_shifted) / scale_parameter)\n",
    "#     return tf.reduce_sum(v, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chan_from_spectrum(\n",
    "    n_coherence,\n",
    "    n_antennas,\n",
    "    angles,\n",
    "    weights,\n",
    "    sigma=2.0,\n",
    "    rng=np.random.default_rng()\n",
    "):\n",
    "\n",
    "    o_f = 100  # oversampling factor (ideally, would use continuous freq. spectrum...)\n",
    "    n_freq_samples = o_f * n_antennas\n",
    "\n",
    "    # Sample the spectrum which is defined in equation (78) with epsilon, try\n",
    "    # to avoid sampling at -pi and pi, thus avoiding dividing by zero.\n",
    "    epsilon = 1 / 3\n",
    "    lattice = np.arange(epsilon, n_freq_samples+epsilon) / n_freq_samples * 2 * np.pi - np.pi #sampled between -pi,+pi\n",
    "    fs = spectrum(lattice, angles, weights, sigma)\n",
    "    fs = np.reshape(fs, [len(fs), 1])\n",
    "\n",
    "    # Avoid instabilities due to almost infinite energy at some frequencies\n",
    "    # (this should only happen at \"endfire\" of a uniform linear array where --\n",
    "    # because of the arcsin-transform -- the angular psd grows to infinity).\n",
    "    almost_inf_threshold = np.max([1, n_freq_samples])  # use n_freq_samples as threshold value...\n",
    "    almost_inf_freqs = np.absolute(fs) > almost_inf_threshold\n",
    "    # this should not/only rarely be entered due to the epsilon above; one might even want to increase the threshold\n",
    "    # to, e.g., 30 * almost_inf_threshold\n",
    "\n",
    "    # if any(np.absolute(fs) > 20 * almost_inf_threshold):\n",
    "    #     print(\"almost inf: \", fs[almost_inf_freqs])\n",
    "\n",
    "    fs[almost_inf_freqs] = almost_inf_threshold  # * np.exp(1j * np.angle(fs[almost_inf_freqs])) # only real values\n",
    "\n",
    "    if np.sum(fs) > 0:\n",
    "        fs = fs / np.sum(fs) * n_freq_samples  # normalize energy\n",
    "\n",
    "    x = crandn(n_freq_samples, n_coherence, rng=rng)\n",
    "\n",
    "    h = np.fft.ifft(np.sqrt(fs)*x, axis=0) * np.sqrt(n_freq_samples)\n",
    "    h = h[0:n_antennas, :]\n",
    "\n",
    "    # t is the first row of the covariance matrix of h (which is Toeplitz and Hermitian)\n",
    "    t = np.fft.fft(fs, axis=0) / n_freq_samples\n",
    "    t = t[0:n_antennas]\n",
    "    t = np.reshape(t, n_antennas)\n",
    "\n",
    "    return h.T, t\n",
    "\n",
    "\n",
    "# def chan_from_spectrum(\n",
    "#     n_coherence,\n",
    "#     n_antennas,\n",
    "#     angles,\n",
    "#     weights,\n",
    "#     sigma=2.0,\n",
    "#     rng=None\n",
    "# ):\n",
    "#     # Set random seed for TensorFlow\n",
    "\n",
    "#     if rng is None:\n",
    "#         tf.random.set_seed(1235428719812346)\n",
    "#         rng = tf.random.Generator.from_seed()\n",
    "        \n",
    "\n",
    "#     o_f = 100  # oversampling factor (ideally, would use continuous freq. spectrum...)\n",
    "#     n_freq_samples = o_f * n_antennas\n",
    "\n",
    "#     # Sample the spectrum using TensorFlow\n",
    "#     epsilon = 1 / 3\n",
    "#     lattice = tf.range(epsilon, n_freq_samples + epsilon, dtype=tf.float32) / n_freq_samples * 2 * np.pi - np.pi\n",
    "#     fs = spectrum(lattice, angles, weights, sigma)  # Assuming you have a spectrum function\n",
    "    \n",
    "#     # Reshape fs\n",
    "#     fs = tf.reshape(fs, [len(fs), 1])\n",
    "\n",
    "#     # Avoid instabilities due to almost infinite energy at some frequencies\n",
    "#     almost_inf_threshold = tf.maximum(1.0, tf.cast(n_freq_samples, dtype=tf.float32))\n",
    "#     almost_inf_freqs = tf.abs(fs) > almost_inf_threshold\n",
    "\n",
    "#     # Set values for almost infinite frequencies\n",
    "#     fs = tf.where(almost_inf_freqs, almost_inf_threshold, fs)\n",
    "\n",
    "#     # Normalize energy\n",
    "#     fs = fs / tf.reduce_sum(fs) * n_freq_samples\n",
    "\n",
    "#     # Generate complex random numbers in TensorFlow\n",
    "#     x = crandn(n_freq_samples, n_coherence, rng=rng)\n",
    "\n",
    "#     print('dtype of tf.cast(tf.math.sqrt(n_freq_sampöles), dtype=tf.complex64): ', tf.math.sqrt(tf.cast(n_freq_samples, dtype=tf.complex64)).dtype)\n",
    "#     print('dtype of x: ', x.dtype)\n",
    "#     print('dtype of multiplication: ', (tf.cast(tf.math.sqrt(fs), dtype=tf.complex64) * x).dtype)\n",
    "\n",
    "#     # Compute channel response in TensorFlow\n",
    "#     h = tf.signal.ifft(tf.cast(tf.math.sqrt(fs), dtype=tf.complex64) * x) * tf.math.sqrt(tf.cast(n_freq_samples, dtype=tf.complex64))\n",
    "#     h = h[0:n_antennas, :]\n",
    "\n",
    "#     # Compute t as the first row of the covariance matrix of h\n",
    "#     t = tf.signal.fft(tf.cast(fs, dtype=tf.complex64)) / n_freq_samples\n",
    "#     t = t[0:n_antennas]\n",
    "#     t = tf.reshape(t, [n_antennas])\n",
    "\n",
    "#     return tf.transpose(h), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCMMulti:\n",
    "    \"\"\"Class to build a multi path channel model.\n",
    "\n",
    "    This class defines a multi path channel model.\n",
    "\n",
    "    Public Methods:\n",
    "\n",
    "    Instance Variables:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path_sigma=2.0, n_path=3):\n",
    "        \"\"\"Initialize multi path channel model.\n",
    "\n",
    "        First, initialise all variables belonging to the multi path channel model.\n",
    "        \"\"\"\n",
    "        self.path_sigma = path_sigma\n",
    "        self.n_path = n_path\n",
    "\n",
    "    def generate_channel(\n",
    "        self,\n",
    "        n_batches,\n",
    "        n_coherence,\n",
    "        n_antennas,\n",
    "        rng=np.random.default_rng()\n",
    "    ):\n",
    "        \"\"\"Generate multi path model parameters.\n",
    "\n",
    "        Returns:\n",
    "            A tuple (h, t) consisting of channels h with\n",
    "                h.shape = (n_batches, n_coherence, n_antennas)\n",
    "            and the first rows t of the covariance matrices with\n",
    "                t.shape = (n_batches, n_antennas)\n",
    "        \"\"\"\n",
    "        h = np.zeros([n_batches, n_coherence, n_antennas], dtype=np.complex64)\n",
    "        t = np.zeros([n_batches, n_antennas], dtype=np.complex64)\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            gains = rng.random(self.n_path)\n",
    "            gains = gains / np.sum(gains, axis=0)\n",
    "            angles = (rng.random(self.n_path) - 0.5) * 180\n",
    "\n",
    "            h[i, :, :], t[i, :] = scm_channel(angles, gains, n_coherence, n_antennas, self.path_sigma, rng=rng)\n",
    "\n",
    "        return h, t\n",
    "\n",
    "    # def generate_channel(\n",
    "    #     self,\n",
    "    #     n_batches,\n",
    "    #     n_coherence,\n",
    "    #     n_antennas,\n",
    "    #     rng=None\n",
    "    # ):\n",
    "\n",
    "    #     if rng is None:\n",
    "    #         tf.random.set_seed(1235428719812346)\n",
    "    #         rng = tf.random.Generator.from_seed()\n",
    "        \n",
    "    #     # n_batches is a symbolic tensor and i want to convert it to a tf.Tensor with the value of batch_size that i define in the main function\n",
    "    #     n_batches = tf.convert_to_tensor(n_batches, dtype=tf.int32)\n",
    "\n",
    "    #     print('n_batches: ', n_batches)\n",
    "\n",
    "    #     h = tf.zeros([n_batches, n_coherence, n_antennas], dtype=tf.complex64)\n",
    "    #     t = tf.zeros([n_batches, n_antennas], dtype=tf.complex64)\n",
    "\n",
    "    #     for i in range(n_batches):\n",
    "    #         gains = rng.uniform(shape=(self.n_path,))\n",
    "    #         gains = gains / tf.reduce_sum(gains)\n",
    "    #         angles = (rng.uniform(shape=(self.n_path,)) - 0.5) * 180.0\n",
    "\n",
    "    #         # Assuming you have a scm_channel function, replace with your actual implementation\n",
    "    #         h_i, t_i = scm_channel(angles, gains, n_coherence, n_antennas, self.path_sigma, rng=rng)\n",
    "\n",
    "    #         print('h_i: ', h_i.shape)\n",
    "    #         print('h: ', h.shape)\n",
    "    #         print('t_i: ', t_i.shape)\n",
    "\n",
    "    #         h = tf.tensor_scatter_nd_add(h, [[i]], h_i)\n",
    "    #         t = tf.tensor_scatter_nd_add(t, [[i]], t_i)\n",
    "\n",
    "    #     return h, t\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'path_sigma': self.path_sigma,\n",
    "            'n_path': self.n_path\n",
    "        }\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import toeplitz\n",
    "def channel_generation(batch_size, n_coherences, n_antennas):\n",
    "    \"\"\"\n",
    "    SIMO version.\n",
    "    \"\"\"\n",
    "    path_sigma = 2.0\n",
    "    n_path = 256\n",
    "    channel = SCMMulti(path_sigma=path_sigma, n_path=n_path)\n",
    "\n",
    "    # generate channel samples with a certain batch size\n",
    "    rng = np.random.default_rng(1235428719812346)\n",
    "\n",
    "    h, t = channel.generate_channel(batch_size, n_coherences, n_antennas, rng)\n",
    "        \n",
    "    # Initialize an empty array to store the covariance matrices\n",
    "    C = np.empty((batch_size, n_antennas, n_antennas), dtype=np.complex64)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Calculate the covariance matrix for each sample\n",
    "        C[i, :, :] = toeplitz(t[i, :])\n",
    "    \n",
    "    # print('Generated ' + str(batch_size) + ' SIMO channel samples of size ' + str(n_antennas) + 'x1.')\n",
    "    \n",
    "    return h, C\n",
    "\n",
    "\n",
    "# def channel_generation(batch_size, n_coherences, n_antennas, seed=1235428719812346):\n",
    "#     path_sigma = 2.0\n",
    "#     n_path = 256\n",
    "#     # Set random seed for TensorFlow\n",
    "#     tf.random.set_seed(seed)\n",
    "\n",
    "#     # Create SCMMulti instance\n",
    "#     channel = SCMMulti(path_sigma=path_sigma, n_path=n_path)\n",
    "\n",
    "#     # Generate channel samples with a certain batch size\n",
    "#     rng = tf.random.Generator.from_seed(seed)\n",
    "#     h, t = channel.generate_channel(batch_size, n_coherences, n_antennas, rng=rng)\n",
    "\n",
    "#     # Initialize an empty array to store the covariance matrices\n",
    "#     C = tf.zeros((batch_size, n_antennas, n_antennas), dtype=tf.complex64)\n",
    "\n",
    "#     for i in range(batch_size):\n",
    "#         # Calculate the covariance matrix for each sample\n",
    "#         t_i = t[i, :]\n",
    "#         C_i = tf.linalg.toeplitz(t_i)\n",
    "#         C = tf.tensor_scatter_nd_add(C, [[i]], tf.expand_dims(C_i, axis=0))\n",
    "\n",
    "#     print('Generated ' + str(batch_size) + ' SIMO channel samples of size ' + str(n_antennas) + 'x1.')\n",
    "\n",
    "#     return h, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 09:29:50.707304: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-07 09:29:50.737561: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-07 09:29:51.148419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class cond_normal_channel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __call__(self, x, no, batch_size, n_coherence, n_antennas, h=None, C=None):\n",
    "        if h is None or C is None:\n",
    "            h, C = channel_generation(batch_size, n_coherence, n_antennas)\n",
    "                        \n",
    "        n = tf.TensorArray(tf.complex64, size=batch_size)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            noise_real_i = tf.random.normal(h[i].shape, dtype=tf.float32)\n",
    "            noise_imag_i = tf.random.normal(h[i].shape, dtype=tf.float32)\n",
    "            noise_i = tf.complex(noise_real_i, noise_imag_i)\n",
    "            noise_i = noise_i * tf.cast(tf.sqrt(no / 2.0), dtype=tf.complex64)\n",
    "            n = n.write(i, noise_i)\n",
    "        n = n.stack()\n",
    "        \n",
    "        x = tf.reshape(x, [-1, 1, 1])\n",
    "                        \n",
    "        y = h * x + n\n",
    "        \n",
    "        #print('first 10 elements of y: ', y[0,0,:10])\n",
    "                \n",
    "        return y, h, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class equalizer(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, h_hat, y, no):\n",
    "               \n",
    "        norm_h_hat_squared = tf.reduce_sum(tf.square(tf.abs(h_hat)), axis=-1)\n",
    "                        \n",
    "        no_new = tf.math.divide_no_nan(\n",
    "            no,\n",
    "            norm_h_hat_squared\n",
    "        )\n",
    "                        \n",
    "        inner_product_h_y = tf.reduce_sum(tf.matmul(y, tf.linalg.adjoint(h_hat)), axis=-1)\n",
    "                                                \n",
    "        x_hat = tf.math.divide_no_nan(\n",
    "            inner_product_h_y,\n",
    "            tf.cast(norm_h_hat_squared, dtype=tf.complex64)\n",
    "        )\n",
    "                        \n",
    "        return x_hat, no_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class genie_mmse_estimator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, y, no, C, pilot):\n",
    "        \n",
    "        #print('no: ', no)\n",
    "                                \n",
    "        # noise_var = no^2 * I. Be careful of the data types!\n",
    "        noise_var = tf.cast(no * tf.eye(C.shape[-1], batch_shape=[C.shape[0]]), dtype=tf.complex64)\n",
    "                                        \n",
    "        # scaled_C = |p|^2 * C\n",
    "        scaled_C = tf.cast(tf.math.abs(pilot) ** 2, dtype=tf.complex64) * tf.cast(C, dtype=tf.complex64)\n",
    "        \n",
    "        # print('check if scaled_C is hermitian', tf.reduce_all(tf.equal(scaled_C, tf.math.conj(tf.transpose(scaled_C, perm=[0, 2, 1])))))\n",
    "        \n",
    "        eigenvalues, eigenvectors = tf.linalg.eigh(scaled_C)\n",
    "                        \n",
    "        # Compute the inverse of (Lambda * noise_var)^-1\n",
    "        inverse_lambda_noise_var = tf.linalg.inv(tf.linalg.diag(eigenvalues) + noise_var)\n",
    "\n",
    "        # Compute the inverse of the sum\n",
    "        # inverse = tf.matmul(tf.matmul(eigenvectors, inverse_lambda_noise_var), tf.transpose(eigenvectors, conjugate=True, perm=[0, 2, 1]))\n",
    "        \n",
    "        # print('shape of einsum of inverse_lambda_noise_var: ', tf.einsum('ijk,ilk->ijl', inverse_lambda_noise_var, tf.transpose(eigenvectors, conjugate=True, perm=[0, 1, 2])).shape)\n",
    "        \n",
    "        # inverse = tf.einsum('ijk,ikl->ijl', eigenvectors, tf.einsum('ijk,ilk->ijl', inverse_lambda_noise_var, tf.transpose(eigenvectors, conjugate=True, perm=[0, 1, 2])))\n",
    "\n",
    "        inverse = tf.linalg.inv(scaled_C + noise_var)\n",
    "        \n",
    "        # print('where are inverse and inverse_2 the same: ', tf.reduce_sum(tf.where(tf.abs(inverse - inverse_2) > 1e-2, tf.ones_like(inverse), tf.zeros_like(inverse))))\n",
    "        \n",
    "        # print('eigenvalues: ', eigenvalues[0, :])\n",
    "        # print('maximum value of eigenvalues: ', tf.math.reduce_max(tf.math.real(eigenvalues), axis=1)[0])\n",
    "        # print('minimum value of eigenvalues: ', tf.math.reduce_min(tf.math.real(eigenvalues), axis=1)[0])          \n",
    "                \n",
    "        # scaled_C_2 = conj(p) * C\n",
    "        scaled_C_2 = tf.math.conj(pilot) * tf.cast(C, dtype=tf.complex64)                        \n",
    "\n",
    "        # matrix = scaled_C_2 * inverse\n",
    "        matrix = tf.matmul(scaled_C_2, inverse)\n",
    "\n",
    "\n",
    "\n",
    "                                                                                                                            \n",
    "        # h_hat_mmse = (scaled_C_2 * inverse) * y. Be careful of the data types!\n",
    "        h_hat_mmse = tf.matmul(matrix, tf.transpose(tf.cast(y, dtype=tf.complex64), perm=[0, 2, 1]))   \n",
    "        h_hat_mmse_2 = tf.einsum('ijk,ilk->ijl', matrix, y) \n",
    "\n",
    "        # print('shape of y: ', y.shape)\n",
    "        # print('shape of h_hat_mmse: ', h_hat_mmse.shape)\n",
    "\n",
    "        # print('first 10 elements of y: ', tf.transpose(tf.cast(y, dtype=tf.complex64), perm=[0, 2, 1])[0, :10, 0])\n",
    "        # print('first 10 elements of h_hat_mmse: ', h_hat_mmse[0, :10, 0])\n",
    "        # print('first 10 dimensions of matrix: ', matrix[0, :10, :10])\n",
    "\n",
    "        #compare h_hat_mmse and y\n",
    "        # print('where are h_hat_mmse and y the same', tf.reduce_sum(tf.where(tf.abs(tf.transpose(h_hat_mmse, perm=[0, 2, 1]) - y) > 1, tf.ones_like(h_hat_mmse), tf.zeros_like(h_hat_mmse))))\n",
    "        \n",
    "        \n",
    "        # print('where are h_hat_mmse and h_hat_mmse_2 the same', tf.reduce_sum(tf.where(tf.abs(h_hat_mmse - h_hat_mmse_2) > 1e-2, tf.ones_like(h_hat_mmse), tf.zeros_like(h_hat_mmse))))\n",
    "                                                \n",
    "        return tf.transpose(h_hat_mmse, perm=[0, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ls_estimator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, y, x):\n",
    "        h_hat_ls = tf.math.divide_no_nan(y, x)\n",
    "                \n",
    "        return h_hat_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 09:29:51.828394: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import sionna as sn\n",
    "except AttributeError:\n",
    "    import sionna as sn\n",
    "\n",
    "\n",
    "class end2endModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_bits_per_symbol, block_length, n_coherence, n_antennas, genie_estimator):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.genie_estimator = genie_estimator\n",
    "        \n",
    "        self.n_coherence = n_coherence\n",
    "        self.n_antennas = n_antennas\n",
    "        self.num_bits_per_symbol = num_bits_per_symbol\n",
    "        self.block_length = block_length\n",
    "        self.constellation = sn.mapping.Constellation(\"qam\", self.num_bits_per_symbol)\n",
    "        self.mapper = sn.mapping.Mapper(constellation=self.constellation)\n",
    "        self.demapper = sn.mapping.Demapper(\"app\", constellation=self.constellation)\n",
    "        self.binary_source = sn.utils.BinarySource()\n",
    "        self.channel = cond_normal_channel()\n",
    "        \n",
    "        self.ls_estimator = ls_estimator()\n",
    "        self.mmse_estimator = genie_mmse_estimator()\n",
    "        \n",
    "        self.equalizer = equalizer()\n",
    "        \n",
    "        self.demapper = sn.mapping.Demapper(\"app\", constellation=self.constellation, num_bits_per_symbol=self.num_bits_per_symbol)\n",
    "\n",
    "    def __call__(self, batch_size, ebno_db):\n",
    "        \n",
    "        #pilot phase\n",
    "        \n",
    "        no = sn.utils.ebnodb2no(ebno_db,\n",
    "                                num_bits_per_symbol=self.num_bits_per_symbol,\n",
    "                                coderate=1.0)\n",
    "        \n",
    "        #print('value of no: ', no)\n",
    "\n",
    "        batch_size = tf.constant(256, dtype=tf.int32)\n",
    "\n",
    "        batch_size = tf.cast(batch_size, dtype=tf.int32)\n",
    "\n",
    "        bits = self.binary_source([batch_size, self.block_length]) # Blocklength set to 1024 bits\n",
    "        \n",
    "        x = self.mapper(bits)\n",
    "        \n",
    "        # print('shape of x: ', x.shape)\n",
    "                                \n",
    "        pilot = tf.ones((batch_size, 1, 1), dtype=tf.complex64)\n",
    "                        \n",
    "        y_p, h, C = self.channel(pilot, no, batch_size, self.n_coherence, self.n_antennas)\n",
    "              \n",
    "        h_hat_ls = self.ls_estimator(y_p, pilot)\n",
    "\n",
    "        h_hat_mmse = self.mmse_estimator(y_p, no, C, pilot)\n",
    "        \n",
    "        \n",
    "                \n",
    "        # print('difference between h_hat_ls and h: ', tf.reduce_sum(tf.abs(h_hat_ls - h)))\n",
    "        # print('difference between h_hat_mmse and h: ', tf.reduce_sum(tf.abs(h_hat_mmse - tf.cast(h, dtype=tf.complex64))))\n",
    "                                \n",
    "        #uplink phase\n",
    "                \n",
    "        #x_data = all x except x[0][0] (x has shape (batch_size, block_length / num_bits_per_symbol))\n",
    "        x_data = x[:, 1:]\n",
    "        \n",
    "        # print('shape of x_data: ', x_data.shape)\n",
    "                                        \n",
    "        y = []\n",
    "        x_hat_ls = []\n",
    "        x_hat_mmse = []\n",
    "        no_ls_new = []\n",
    "        no_mmse_new = []\n",
    "        llr_ls = tf.TensorArray(dtype=tf.float32, size=tf.cast(self.block_length / self.num_bits_per_symbol - 1, dtype=tf.int32))\n",
    "        llr_mmse = tf.TensorArray(dtype=tf.float32, size=tf.cast(self.block_length / self.num_bits_per_symbol - 1, dtype=tf.int32))\n",
    "                \n",
    "        for i in range(tf.shape(x_data)[1]):\n",
    "            #y = h * x + n for all x except first one\n",
    "            # print('x_data[0][i].shape: ', x_data[0][i].shape)\n",
    "            y_i = self.channel(x_data[:, i], no, batch_size, self.n_coherence, self.n_antennas, h, C)[0]\n",
    "            # print('y_i.shape: ', y_i.shape)\n",
    "            y.append(y_i)\n",
    "        \n",
    "            x_hat_ls_i, no_ls_new_i = self.equalizer(h_hat_ls, y_i, no)\n",
    "            # print('x_hat_ls_i.shape: ', x_hat_ls_i.shape)\n",
    "            # print('no_ls_new_i.shape: ', no_ls_new_i.shape)\n",
    "            x_hat_ls.append(x_hat_ls_i)\n",
    "            no_ls_new.append(no_ls_new_i)\n",
    "            \n",
    "            # print('difference between x_hat_ls_i and x_data[:, i]: ', tf.reduce_sum(tf.abs(x_hat_ls_i - tf.reshape(x_data[:, i], [-1, 1]))))\n",
    "\n",
    "            x_hat_mmse_i, no_mmse_new_i = self.equalizer(h_hat_mmse, y_i, no)\n",
    "            x_hat_mmse.append(x_hat_mmse_i)\n",
    "            no_mmse_new.append(no_mmse_new_i)\n",
    "                        \n",
    "            # print('difference between x_hat_mmse_i and x_data[:, i]: ', tf.reduce_sum(tf.abs(x_hat_mmse_i - tf.reshape(x_data[:, i], [-1, 1]))))\n",
    "                        \n",
    "            #print('value of x_hat_mmse_i: ', x_hat_mmse_i)\n",
    "            #print('value of no_mmse_new_i: ', no_mmse_new_i[0])\n",
    "            \n",
    "            llr_ls_i = self.demapper([x_hat_ls_i, no_ls_new_i])\n",
    "            #llr_ls = tf.concat([llr_ls, tf.reshape(llr_ls_i, (batch_size, 2, 1))], axis=2)\n",
    "            llr_ls = llr_ls.write(i, llr_ls_i)\n",
    "            \n",
    "            llr_mmse_i = self.demapper([x_hat_mmse_i, no_mmse_new_i])\n",
    "            # print('llr_mmse_i.shape: ', llr_mmse_i.shape)\n",
    "            llr_mmse = llr_mmse.write(i, llr_mmse_i)\n",
    "            #llr_mmse = tf.concat([llr_mmse, tf.reshape(llr_mmse_i, (batch_size, 2, 1))], axis=2)\n",
    "        \n",
    "         \n",
    "        bits = bits[:, self.num_bits_per_symbol:]\n",
    "        \n",
    "        llr_ls = llr_ls.stack()\n",
    "        #print('llr_ls first two elements: ', llr_ls[0][0][0], llr_ls[0][0][1])\n",
    "        # print('shape of bits: ', bits.shape)\n",
    "        llr_ls = tf.transpose(llr_ls, perm=[1, 0, 2])\n",
    "        #print('llrs_ls.shape after transpose: ', llr_ls.shape)\n",
    "        llr_mmse = llr_mmse.stack()\n",
    "        # print('llr_mmse.shape: ', llr_mmse.shape)\n",
    "        llr_mmse = tf.transpose(llr_mmse, perm=[1, 0, 2])\n",
    "        #print('llrs_mmse.shape after transpose: ', llr_mmse.shape)\n",
    "        \n",
    "        \n",
    "        llr_ls = tf.split(llr_ls, num_or_size_splits=2, axis=2)\n",
    "        llr_mmse = tf.split(llr_mmse, num_or_size_splits=2, axis=2)\n",
    "            \n",
    "        \n",
    "        llr_ls = tf.reshape(tf.stack(llr_ls, axis=2), bits.shape)\n",
    "        llr_mmse = tf.reshape(tf.stack(llr_mmse, axis=2), bits.shape)\n",
    "        \n",
    "        # bits_hat = tf.where(llr_ls > 0.0, tf.ones_like(bits), tf.zeros_like(bits))\n",
    "        \n",
    "        # print(\"bits_hat before reshaping:\\n\", bits_hat[:3, :6])\n",
    "        # #print(\"llr_mmse before reshaping:\\n\", llr_mmse[:3, :3, :])\n",
    "        # print(\"bits:\\n\", bits[:3, :6])  # 6 columns to account for interleaving\n",
    "        #print('shape of llr_ls after reshape: ', llr_ls.shape)\n",
    "        #print('llr_ls first two elements after reshape: ', llr_ls[0][0], llr_ls[0][1])\n",
    "        # print('shape of llr_mmse after reshape: ', llr_mmse.shape)\n",
    "        \n",
    "        # bits_hat = tf.where(llr_ls > 0.0, tf.ones_like(bits), tf.zeros_like(bits))\n",
    "        \n",
    "        # print('first 30 bits in bits_hat: ', bits_hat[1][:30])\n",
    "        # print('first 30 bits in bits: ', bits[1][:30])\n",
    "        \n",
    "        # mismatched_indices = tf.where(tf.not_equal(bits, bits_hat))\n",
    "        # print(mismatched_indices[100: 150])\n",
    "\n",
    "                        \n",
    "        \n",
    "        if self.genie_estimator:\n",
    "            return bits, llr_mmse\n",
    "        \n",
    "        return bits, llr_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -10.0 | 1.6133e-01 | 1.0000e+00 |        5204 |       32256 |          256 |         256 |        11.3 |reached target block errors\n",
      "   -9.592 | 1.5002e-01 | 9.8828e-01 |        4839 |       32256 |          253 |         256 |        10.5 |reached target block errors\n",
      "   -9.184 | 1.2602e-01 | 9.8438e-01 |        4065 |       32256 |          252 |         256 |        10.2 |reached target block errors\n",
      "   -8.776 | 1.0420e-01 | 9.7266e-01 |        3361 |       32256 |          249 |         256 |        10.2 |reached target block errors\n",
      "   -8.367 | 9.6416e-02 | 9.6875e-01 |        3110 |       32256 |          248 |         256 |        10.2 |reached target block errors\n",
      "   -7.959 | 8.1566e-02 | 9.5703e-01 |        2631 |       32256 |          245 |         256 |         9.9 |reached target block errors\n",
      "   -7.551 | 6.3523e-02 | 9.0234e-01 |        2049 |       32256 |          231 |         256 |        10.1 |reached target block errors\n",
      "   -7.143 | 5.5122e-02 | 8.7891e-01 |        1778 |       32256 |          225 |         256 |        10.1 |reached target block errors\n",
      "   -6.735 | 3.9249e-02 | 7.7734e-01 |        1266 |       32256 |          199 |         256 |        10.0 |reached target block errors\n",
      "   -6.327 | 3.4381e-02 | 7.3438e-01 |        1109 |       32256 |          188 |         256 |         9.9 |reached target block errors\n",
      "   -5.918 | 2.1453e-02 | 6.6406e-01 |         692 |       32256 |          170 |         256 |        10.0 |reached target block errors\n",
      "    -5.51 | 1.6493e-02 | 5.8984e-01 |         532 |       32256 |          151 |         256 |         9.9 |reached target block errors\n",
      "   -5.102 | 1.1719e-02 | 4.8047e-01 |         378 |       32256 |          123 |         256 |        10.4 |reached target block errors\n",
      "   -4.694 | 8.6496e-03 | 4.0625e-01 |         279 |       32256 |          104 |         256 |        10.0 |reached target block errors\n",
      "   -4.286 | 6.1074e-03 | 3.0469e-01 |         394 |       64512 |          156 |         512 |        20.3 |reached target block errors\n",
      "   -3.878 | 3.2707e-03 | 2.0117e-01 |         211 |       64512 |          103 |         512 |        20.0 |reached target block errors\n",
      "   -3.469 | 2.2735e-03 | 1.5625e-01 |         220 |       96768 |          120 |         768 |        30.3 |reached target block errors\n",
      "   -3.061 | 1.3269e-03 | 9.5312e-02 |         214 |      161280 |          122 |        1280 |        49.9 |reached target block errors\n",
      "   -2.653 | 8.4222e-04 | 6.6406e-02 |         163 |      193536 |          102 |        1536 |        61.3 |reached target block errors\n",
      "   -2.245 | 4.4953e-04 | 3.6328e-02 |         145 |      322560 |           93 |        2560 |        99.9 |reached max iter       \n",
      "   -1.837 | 1.9221e-04 | 1.8359e-02 |          62 |      322560 |           47 |        2560 |       100.0 |reached max iter       \n",
      "   -1.429 | 4.6503e-05 | 5.4687e-03 |          15 |      322560 |           14 |        2560 |       101.7 |reached max iter       \n",
      "    -1.02 | 2.1701e-05 | 2.7344e-03 |           7 |      322560 |            7 |        2560 |       101.0 |reached max iter       \n",
      "   -0.612 | 1.8601e-05 | 1.9531e-03 |           6 |      322560 |            5 |        2560 |       100.4 |reached max iter       \n",
      "   -0.204 | 3.1002e-06 | 3.9063e-04 |           1 |      322560 |            1 |        2560 |       100.3 |reached max iter       \n",
      "    0.204 | 3.1002e-06 | 3.9063e-04 |           1 |      322560 |            1 |        2560 |       101.1 |reached max iter       \n",
      "    0.612 | 3.1002e-06 | 3.9063e-04 |           1 |      322560 |            1 |        2560 |        99.7 |reached max iter       \n",
      "     1.02 | 0.0000e+00 | 0.0000e+00 |           0 |      322560 |            0 |        2560 |        99.9 |reached max iter       \n",
      "\n",
      "Simulation stopped as no error occurred @ EbNo = 1.0 dB.\n",
      "\n",
      "EbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -10.0 | 1.6806e-01 | 1.0000e+00 |        5421 |       32256 |          256 |         256 |        10.0 |reached target block errors\n",
      "   -9.592 | 1.6214e-01 | 1.0000e+00 |        5230 |       32256 |          256 |         256 |         9.9 |reached target block errors\n",
      "   -9.184 | 1.4038e-01 | 1.0000e+00 |        4528 |       32256 |          256 |         256 |         9.9 |reached target block errors\n",
      "   -8.776 | 1.1465e-01 | 9.9609e-01 |        3698 |       32256 |          255 |         256 |        10.0 |reached target block errors\n",
      "   -8.367 | 1.0302e-01 | 9.8828e-01 |        3323 |       32256 |          253 |         256 |         9.8 |reached target block errors\n",
      "   -7.959 | 8.0543e-02 | 9.8047e-01 |        2598 |       32256 |          251 |         256 |         9.9 |reached target block errors\n",
      "   -7.551 | 7.1460e-02 | 9.3750e-01 |        2305 |       32256 |          240 |         256 |        10.1 |reached target block errors\n",
      "   -7.143 | 5.5990e-02 | 9.0625e-01 |        1806 |       32256 |          232 |         256 |         9.9 |reached target block errors\n",
      "   -6.735 | 4.1884e-02 | 8.3594e-01 |        1351 |       32256 |          214 |         256 |        10.0 |reached target block errors\n",
      "   -6.327 | 3.2800e-02 | 8.1641e-01 |        1058 |       32256 |          209 |         256 |         9.9 |reached target block errors\n",
      "   -5.918 | 2.3593e-02 | 7.2266e-01 |         761 |       32256 |          185 |         256 |         9.9 |reached target block errors\n",
      "    -5.51 | 1.7640e-02 | 5.9375e-01 |         569 |       32256 |          152 |         256 |         9.9 |reached target block errors\n",
      "   -5.102 | 1.2091e-02 | 4.8828e-01 |         390 |       32256 |          125 |         256 |        10.6 |reached target block errors\n",
      "   -4.694 | 8.6806e-03 | 3.9062e-01 |         280 |       32256 |          100 |         256 |        14.9 |reached target block errors\n",
      "   -4.286 | 5.4719e-03 | 3.1641e-01 |         353 |       64512 |          162 |         512 |        20.5 |reached target block errors\n",
      "   -3.878 | 4.0148e-03 | 2.2852e-01 |         259 |       64512 |          117 |         512 |        20.4 |reached target block errors\n",
      "   -3.469 | 2.3665e-03 | 1.3802e-01 |         229 |       96768 |          106 |         768 |        30.5 |reached target block errors\n",
      "   -3.061 | 1.3718e-03 | 1.0547e-01 |         177 |      129024 |          108 |        1024 |        40.3 |reached target block errors\n",
      "   -2.653 | 7.4848e-04 | 5.8036e-02 |         169 |      225792 |          104 |        1792 |        70.8 |reached target block errors\n",
      "   -2.245 | 4.0613e-04 | 3.5937e-02 |         131 |      322560 |           92 |        2560 |       101.2 |reached max iter       \n",
      "   -1.837 | 1.6741e-04 | 1.7969e-02 |          54 |      322560 |           46 |        2560 |       101.7 |reached max iter       \n",
      "   -1.429 | 7.4405e-05 | 8.2031e-03 |          24 |      322560 |           21 |        2560 |       101.4 |reached max iter       \n",
      "    -1.02 | 6.2004e-05 | 5.0781e-03 |          20 |      322560 |           13 |        2560 |       101.4 |reached max iter       \n",
      "   -0.612 | 7.7505e-06 | 9.7656e-04 |           1 |      129024 |            1 |        1024 |        43.2 |iter: 3/10\r"
     ]
    }
   ],
   "source": [
    "num_bits_per_symbol = 2\n",
    "block_length = 128\n",
    "ebno_db_min = -10.0 # Minimum value of Eb/N0 [dB] for simulations\n",
    "ebno_db_max = 10.0 # Maximum value of Eb/N0 [dB] for simulations\n",
    "batch_size = 128 # How many examples are processed by Sionna in parallel\n",
    "n_coherence = 1\n",
    "n_antennas = 32\n",
    "\n",
    "uncoded_mmse_model = end2endModel(num_bits_per_symbol=num_bits_per_symbol, block_length=block_length, n_coherence=n_coherence, n_antennas=n_antennas, genie_estimator=True)\n",
    "    \n",
    "ber_plots = sn.utils.PlotBER(\"SC over 3GPP channel\")\n",
    "ber_plots.simulate(\n",
    "    uncoded_mmse_model,\n",
    "    ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 50),\n",
    "    batch_size=batch_size,\n",
    "    num_target_block_errors=100, # simulate until 100 block errors occured\n",
    "    legend=\"Genie MMSE\",\n",
    "    soft_estimates=True,\n",
    "    max_mc_iter=10, # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "    show_fig=False   \n",
    ")\n",
    "\n",
    "uncoded_ls_model = end2endModel(num_bits_per_symbol=num_bits_per_symbol, block_length=block_length, n_coherence=n_coherence, n_antennas=n_antennas, genie_estimator=False)\n",
    "\n",
    "ber_plots.simulate(\n",
    "    uncoded_ls_model,\n",
    "    ebno_dbs=np.linspace(ebno_db_min, ebno_db_max, 50),\n",
    "    batch_size=batch_size,\n",
    "    num_target_block_errors=100, # simulate until 100 block errors occured\n",
    "    legend=\"LSE\",\n",
    "    soft_estimates=True,\n",
    "    max_mc_iter=10, # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "    show_fig=False\n",
    ")\n",
    "\n",
    "ber_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
